{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23234b50-e6c6-4c87-9f97-259c15f36894",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Streaming final agent output | 最終エージェントの出力をストリーミング\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd6333-307c-43df-b848-65001c01733b",
   "metadata": {},
   "source": [
    "エージェントの最終出力のみをストリーミングしたい場合は、`FinalStreamingStdOutCallbackHandler` コールバックを使用できます。これを使用するには、基盤となるLLMもストリーミングをサポートしている必要があります。\n",
    "\n",
    "> If you only want the final output of an agent to be streamed, you can use the callback `FinalStreamingStdOutCallbackHandler`.\n",
    "> For this, the underlying LLM has to support streaming as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4592215-6604-47e2-89ff-5db3af6d1e40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.callbacks.streaming_stdout_final_only import (\n",
    "    FinalStreamingStdOutCallbackHandler,\n",
    ")\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a813f7",
   "metadata": {},
   "source": [
    "基礎となるLLMを`streaming = True`で作成し、`FinalStreamingStdOutCallbackHandler`の新しいインスタンスを渡しましょう。\n",
    "\n",
    "> Let's create the underlying LLM with `streaming = True` and pass a new instance of `FinalStreamingStdOutCallbackHandler`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe81ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    streaming=True, callbacks=[FinalStreamingStdOutCallbackHandler()], temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff45b85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Konrad Adenauer became Chancellor of Germany in 1949, 74 years ago in 2023."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Konrad Adenauer became Chancellor of Germany in 1949, 74 years ago in 2023.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False\n",
    ")\n",
    "agent.run(\n",
    "    \"It's 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a743b8",
   "metadata": {},
   "source": [
    "### Handling custom answer prefixes | カスタム回答プレフィックスの処理\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23602c62",
   "metadata": {},
   "source": [
    "デフォルトでは、トークンのシーケンス「Final」「Answer」「:」が、エージェントが答えを出したことを示していると解釈されます。しかし、答えの接頭辞として使用するカスタムシーケンスを指定することもできます。\n",
    "\n",
    "> By default, we assume that the token sequence `\"Final\", \"Answer\", \":\"` indicates that the agent has reached an answers. We can, however, also pass a custom sequence to use as answer prefix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5662a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(\n",
    "    streaming=True,\n",
    "    callbacks=[\n",
    "        FinalStreamingStdOutCallbackHandler(answer_prefix_tokens=[\"The\", \"answer\", \":\"])\n",
    "    ],\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a96cc0",
   "metadata": {},
   "source": [
    "利便性のために、コールバックは`answer_prefix_tokens`と比較する際に自動的に空白や改行文字を除去します。例えば、`answer_prefix_tokens = [\"The\", \" answer\", \":\"]`であれば、`[\"\\nThe\", \" answer\", \":\"]`も`[\"The\", \" answer\", \":\"]`も答えの接頭辞として認識されることになります。\n",
    "\n",
    "> For convenience, the callback automatically strips whitespaces and new line characters when comparing to `answer_prefix_tokens`. I.e., if `answer_prefix_tokens = [\"The\", \" answer\", \":\"]` then both `[\"\\nThe\", \" answer\", \":\"]` and `[\"The\", \" answer\", \":\"]` would be recognized a the answer prefix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278b522",
   "metadata": {},
   "source": [
    "もし回答のプレフィックスのトークン化されたバージョンがわからない場合は、以下のコードでそれを決定することができます：\n",
    "\n",
    "> If you don't know the tokenized version of your answer prefix, you can determine it with the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "\n",
    "class MyCallbackHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token, **kwargs) -> None:\n",
    "        # print every token on a new line\n",
    "        print(f\"#{token}#\")\n",
    "\n",
    "\n",
    "llm = OpenAI(streaming=True, callbacks=[MyCallbackHandler()])\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=False\n",
    ")\n",
    "agent.run(\n",
    "    \"It's 2023 now. How many years ago did Konrad Adenauer become Chancellor of Germany.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61190e58",
   "metadata": {},
   "source": [
    "### Also streaming the answer prefixes | 回答のプレフィックスも同時にストリーミングされます\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255776f",
   "metadata": {},
   "source": [
    "`stream_prefix = True` パラメータが設定されている場合、回答のプレフィックス自体もストリーミングされます。これは、回答のプレフィックス自体が回答の一部となる場合に便利です。例えば、あなたの回答が JSON のような形式である場合に該当します\n",
    "\n",
    "> When the parameter `stream_prefix = True` is set, the answer prefix itself will also be streamed. This can be useful when the answer prefix itself is part of the answer. For example, when your answer is a JSON like\n",
    "\n",
    "{ \"action\": \"最終回答\", \"action\\_input\": \"コンラート・アデナウアーは74年前に首相に就任しました。\" }\n",
    "\n",
    "> `{\n",
    ">     \"action\": \"Final answer\",\n",
    ">     \"action_input\": \"Konrad Adenauer became Chancellor 74 years ago.\"\n",
    "> }`\n",
    "\n",
    "そして、`action_input`だけでなく、JSON全体がストリーミングされることを望んでいます。\n",
    "\n",
    "> and you don't only want the `action_input` to be streamed, but the entire JSON.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}