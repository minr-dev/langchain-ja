{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a7cc773",
   "metadata": {},
   "source": [
    "# Recursive URL | 再帰的URL\n",
    "\n",
    "ルートディレクトリ以下のすべてのURLを処理して読み込むことがあるかもしれません。\n",
    "\n",
    "> We may want to process load all URLs under a root directory.\n",
    "\n",
    "例えば、[Python 3.9のドキュメント](https://docs.python.org/3.9/)を見てみましょう。\n",
    "\n",
    "> For example, let's look at the [Python 3.9 Document](https://docs.python.org/3.9/).\n",
    "\n",
    "これには、一括で読みたくなるような多くの興味深い子ページがあります。\n",
    "\n",
    "> This has many interesting child pages that we may want to read in bulk.\n",
    "\n",
    "もちろん、`WebBaseLoader`はページのリストを読み込むことができます。\n",
    "\n",
    "> Of course, the `WebBaseLoader` can load a list of pages.\n",
    "\n",
    "しかし、課題は子ページのツリーをたどり、実際にそのリストを組み立てることです！\n",
    "\n",
    "> But, the challenge is traversing the tree of child pages and actually assembling that list!\n",
    "\n",
    "`RecursiveUrlLoader`を使用してこれを行います。\n",
    "\n",
    "> We do this using the `RecursiveUrlLoader`.\n",
    "\n",
    "これにより、特定の子要素を除外したり、エクストラクターをカスタマイズしたりするなど、より柔軟性が得られます。\n",
    "\n",
    "> This also gives us the flexibility to exclude some children, customize the extractor, and more.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8094f",
   "metadata": {},
   "source": [
    "# Parameters | パラメータ\n",
    "\n",
    "* url: str, クロール対象のURL。\n",
    "\n",
    "  > url: str, the target url to crawl.\n",
    "\n",
    "* exclude\\_dirs: Optional\\[str]、除外するウェブページのディレクトリ。\n",
    "\n",
    "  > exclude\\_dirs: Optional\\[str], webpage directories to exclude.\n",
    "\n",
    "* use\\_async: Optional\\[bool]は、非同期リクエストを使用するかどうかを指定します。非同期リクエストは通常、大規模なタスクでより速く動作します。ただし、非同期を使用すると、遅延ローディング機能が無効になります（関数は引き続き動作しますが、遅延ローディングは行われません）。デフォルトでは、Falseに設定されています。\n",
    "\n",
    "  > use\\_async: Optional\\[bool], wether to use async requests, using async requests is usually faster in large tasks. However, async will disable the lazy loading feature(the function still works, but it is not lazy). By default, it is set to False.\n",
    "\n",
    "* extractor: Optional\\[Callable\\[\\[str], str]]は、ウェブページからドキュメントのテキストを抽出する関数で、デフォルトではページをそのまま返します。テキストを抽出するためには、goose3やbeautifulsoupのようなツールの使用を推奨します。デフォルトでは、ページをそのまま返すだけです。\n",
    "\n",
    "  > extractor: Optional\\[Callable\\[\\[str], str]], a function to extract the text of the document from the webpage, by default it returns the page as it is. It is recommended to use tools like goose3 and beautifulsoup to extract the text. By default, it just returns the page as it is.\n",
    "\n",
    "* max\\_depth: Optional\\[int] = None、クロールする最大の深さです。デフォルトでは2に設定されています。ウェブサイト全体をクロールする必要がある場合は、十分に大きな数値を設定することで対応できます。\n",
    "\n",
    "  > max\\_depth: Optional\\[int] = None, the maximum depth to crawl. By default, it is set to 2. If you need to crawl the whole website, set it to a number that is large enough would simply do the job.\n",
    "\n",
    "* timeout: Optional\\[int] = None, 各リクエストのタイムアウトを秒単位で設定します。デフォルトでは10に設定されています。\n",
    "\n",
    "  > timeout: Optional\\[int] = None, the timeout for each request, in the unit of seconds. By default, it is set to 10.\n",
    "\n",
    "* prevent\\_outside: Optional\\[bool] = None, ルートURLの外部へのクローリングを防止するかどうかを指定します。デフォルトでは、Trueに設定されています。\n",
    "\n",
    "  > prevent\\_outside: Optional\\[bool] = None, whether to prevent crawling outside the root url. By default, it is set to True.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c18539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6384c057",
   "metadata": {},
   "source": [
    "簡単な例を試してみましょう。\n",
    "\n",
    "> Let's try a simple example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55394afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "url = \"https://docs.python.org/3.9/\"\n",
    "loader = RecursiveUrlLoader(\n",
    "    url=url, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084fb2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nPython Frequently Asked Questions — Python 3.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13bd7e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://docs.python.org/3.9/library/index.html',\n",
       " 'title': 'The Python Standard Library — Python 3.9.17 documentation',\n",
       " 'language': None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[-1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5866e5a6",
   "metadata": {},
   "source": [
    "しかし、完璧なフィルタリングを行うことは難しいため、結果には関連性のないものがいくつか表示されることがあります。必要に応じて、返されたドキュメントに対して自分でフィルタリングを行うことができます。ほとんどの場合、返された結果は十分に良いものです。\n",
    "\n",
    "> However, since it's hard to perform a perfect filter, you may still see some irrelevant results in the results. You can perform a filter on the returned documents by yourself, if it's needed. Most of the time, the returned results are good enough.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8ecef",
   "metadata": {},
   "source": [
    "LangChainのドキュメントでテストを行います。\n",
    "\n",
    "> Testing on LangChain docs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "349b5598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://js.langchain.com/docs/modules/memory/integrations/\"\n",
    "loader = RecursiveUrlLoader(url=url)\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}