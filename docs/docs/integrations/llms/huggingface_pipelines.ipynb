{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "959300d4",
   "metadata": {},
   "source": [
    "# Hugging Face Local Pipelines | Hugging Face ローカルパイプライン\n",
    "\n",
    "Hugging Faceのモデルは、`HuggingFacePipeline` クラスを通じてローカルで実行することができます。\n",
    "\n",
    "> Hugging Face models can be run locally through the `HuggingFacePipeline` class.\n",
    "\n",
    "[Hugging Face Model Hub](https://huggingface.co/models)は、12万以上のモデル、2万以上のデータセット、5万以上のデモアプリ（Spaces）をホストしており、すべてオープンソースで公開されているオンラインプラットフォームです。ここでは、人々が簡単に協力し、共に機械学習を構築できます。\n",
    "\n",
    "> The [Hugging Face Model Hub](https://huggingface.co/models) hosts over 120k models, 20k datasets, and 50k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together.\n",
    "\n",
    "これらは、このローカルパイプラインラッパーを通じて、またはHuggingFaceHubクラスを通じてホストされた推論エンドポイントを呼び出すことによってLangChainから呼び出すことができます。ホストされたパイプラインの詳細については、[HuggingFaceHub](huggingface_hub.html)ノートブックをご覧ください。\n",
    "\n",
    "> These can be called from LangChain either through this local pipeline wrapper or by calling their hosted inference endpoints through the HuggingFaceHub class. For more information on the hosted pipelines, see the [HuggingFaceHub](huggingface_hub.html) notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1b8450-5eaf-4d34-8341-2d785448a1ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "使用するには、`transformers` Pythonパッケージを[インストール](https://pypi.org/project/transformers/)しておく必要があり、また[pytorch](https://pytorch.org/get-started/locally/)もインストールしてください。さらにメモリ効率の良いアテンション実装のために、`xformer`をインストールすることもできます。\n",
    "\n",
    "> To use, you should have the `transformers` python [package installed](https://pypi.org/project/transformers/), as well as [pytorch](https://pytorch.org/get-started/locally/). You can also install `xformer` for a more memory-efficient attention implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772b637-de00-4663-bd77-9bc96d798db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ad075f-71d5-4bc8-ab91-cc0ad5ef16bb",
   "metadata": {},
   "source": [
    "### Model Loading | モデルの読み込み\n",
    "\n",
    "`from_model_id` メソッドを使用してモデルパラメータを指定することで、モデルをロードすることができます。\n",
    "\n",
    "> Models can be loaded by specifying the model parameters using the `from_model_id` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165ae236-962a-4763-8052-c4836d78a5d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00104b27-0c15-4a97-b198-4512337ee211",
   "metadata": {},
   "source": [
    "また、既存の `transformers` パイプラインを直接渡すことで読み込むこともできます\n",
    "\n",
    "> They can also be loaded by passing in an existing `transformers` pipeline directly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ],
   "id": "7f426a4f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chain | チェーンを作成する\n",
    "\n",
    "モデルをメモリにロードしたら、プロンプトと組み合わせてチェーンを形成することができます。\n",
    "\n",
    "> With the model loaded into memory, you can compose it with a prompt to\n",
    "> form a chain.\n",
    "\n"
   ],
   "id": "60e7ba8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acf0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc3a37",
   "metadata": {},
   "source": [
    "### GPU Inference | GPUによる推論\n",
    "\n",
    "GPUを搭載したマシンで実行する際には、`device=n` パラメータを指定して、モデルを特定のデバイスに割り当てることができます。デフォルトはCPUでの推論のため `-1` に設定されています。\n",
    "\n",
    "> When running on a machine with GPU, you can specify the `device=n` parameter to put the model on the specified device.\n",
    "> Defaults to `-1` for CPU inference.\n",
    "\n",
    "複数のGPUを使用している場合や、モデルが単一のGPUに収まりきらないほど大きい場合、`device_map=\"auto\"`を指定することができます。これを使用するには[Accelerate](https://huggingface.co/docs/accelerate/index)ライブラリが必要であり、モデルの重みをどのようにロードするかを自動的に判断します。\n",
    "\n",
    "> If you have multiple-GPUs and/or the model is too large for a single GPU, you can specify `device_map=\"auto\"`, which requires and uses the [Accelerate](https://huggingface.co/docs/accelerate/index) library to automatically determine how to load the model weights.\n",
    "\n",
    "注意：`device`と`device_map`は同時に指定してはいけません。同時に指定すると予期しない動作が発生する可能性があります。\n",
    "\n",
    "> *Note*: both `device` and `device_map` should not be specified together and can lead to unexpected behavior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,  # replace with device_map=\"auto\" to use the accelerate library.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 10},\n",
    ")\n",
    "\n",
    "gpu_chain = prompt | gpu_llm\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(gpu_chain.invoke({\"question\": question}))"
   ],
   "id": "703c91c8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch GPU Inference | バッチ GPU 推論\n",
    "\n",
    "GPUを搭載したデバイスで実行している場合、バッチモードでGPU上で推論を実行することもできます。\n",
    "\n",
    "> If running on a device with GPU, you can also run inference on the GPU in batch mode.\n",
    "\n"
   ],
   "id": "59276016"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ba62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"bigscience/bloom-1b7\",\n",
    "    task=\"text-generation\",\n",
    "    device=0,  # -1 for CPU\n",
    "    batch_size=2,  # adjust as needed based on GPU map and model size.\n",
    "    model_kwargs={\"temperature\": 0, \"max_length\": 64},\n",
    ")\n",
    "\n",
    "gpu_chain = prompt | gpu_llm.bind(stop=[\"\\n\\n\"])\n",
    "\n",
    "questions = []\n",
    "for i in range(4):\n",
    "    questions.append({\"question\": f\"What is the number {i} in french?\"})\n",
    "\n",
    "answers = gpu_chain.batch(questions)\n",
    "for answer in answers:\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}