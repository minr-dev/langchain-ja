{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c613812f",
   "metadata": {},
   "source": [
    "# Gradio | Gradio\n",
    "\n",
    "`Hugging Face Spaces`ã«ã¯æ•°åƒã‚‚ã®`Gradio`ã‚¢ãƒ—ãƒªãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ã‚ˆã‚Šã€ãã‚Œã‚‰ã‚’ã‚ãªãŸã®LLMã®æŒ‡å…ˆã§ç›´æŽ¥æ“ä½œã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ðŸ¦¾\n",
    "\n",
    "> There are many 1000s of `Gradio` apps on `Hugging Face Spaces`. This library puts them at the tips of your LLM's fingers ðŸ¦¾\n",
    "\n",
    "`gradio-tools`ã¯ã€`Gradio`ã‚¢ãƒ—ãƒªã‚’å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒãã®ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã™ã‚‹ãŸã‚ã«æ´»ç”¨ã§ãã‚‹ãƒ„ãƒ¼ãƒ«ã«å¤‰æ›ã™ã‚‹Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ä¾‹ãˆã°ã€LLMã¯`Gradio`ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§è¦‹ã¤ã‘ãŸéŸ³å£°è¨˜éŒ²ã‚’æ›¸ãèµ·ã“ã—ã€ãã‚Œã‚’ã‚ãªãŸã«è¦ç´„ã—ã¦æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã¯ã€Googleãƒ‰ãƒ©ã‚¤ãƒ–ä¸Šã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«OCRã‚’é©ç”¨ã—ã€ãã®å†…å®¹ã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã«åˆ¥ã®`Gradio`ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n",
    "\n",
    "> Specifically, `gradio-tools` is a Python library for converting `Gradio` apps into tools that can be leveraged by a large language model (LLM)-based agent to complete its task. For example, an LLM could use a `Gradio` tool to transcribe a voice recording it finds online and then summarize it for you. Or it could use a different `Gradio` tool to apply OCR to a document on your Google Drive and then answer questions about it.\n",
    "\n",
    "ã‚‚ã—äº‹å‰ã«æ§‹ç¯‰ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã®ä¸­ã«ã”å¸Œæœ›ã®ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„å ´åˆã€è‡ªåˆ†è‡ªèº«ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã™ã‚‹ã“ã¨ã¯éžå¸¸ã«ç°¡å˜ã§ã™ã€‚ãã®æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€gradio-toolsã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®è©²å½“ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã”è¦§ãã ã•ã„ã€‚ã©ã‚“ãªè²¢çŒ®ã‚‚æ­“è¿Žã„ãŸã—ã¾ã™ï¼\n",
    "\n",
    "> It's very easy to create you own tool if you want to use a space that's not one of the pre-built tools. Please see this section of the gradio-tools documentation for information on how to do that. All contributions are welcome!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17608431",
   "metadata": {},
   "source": [
    "## Using a tool | ãƒ„ãƒ¼ãƒ«ã®ä½¿ç”¨\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423f9dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradio_tools.tools import StableDiffusionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b8f077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://gradio-client-demos-stable-diffusion.hf.space âœ”\n",
      "\n",
      "Job Status: Status.STARTING eta: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/harrisonchase/workplace/langchain/docs/modules/agents/tools/integrations/b61c1dd9-47e2-46f1-a47c-20d27640993d/tmp4ap48vnm.jpg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_file_path = StableDiffusionTool().langchain.run(\n",
    "    \"Please create a photo of a dog riding a skateboard\"\n",
    ")\n",
    "local_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7bdfd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e09784",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1e602",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "display(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeeeeb5",
   "metadata": {},
   "source": [
    "## Using within an agent | ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå†…ã§ã®ä½¿ç”¨\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a9d45b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://gradio-client-demos-stable-diffusion.hf.space âœ”\n",
      "Loaded as API: https://taesiri-blip-2.hf.space âœ”\n",
      "Loaded as API: https://microsoft-promptist.hf.space âœ”\n",
      "Loaded as API: https://damo-vilab-modelscope-text-to-video-synthesis.hf.space âœ”\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: StableDiffusionPromptGenerator\n",
      "Action Input: A dog riding a skateboard\u001b[0m\n",
      "Job Status: Status.STARTING eta: None\n",
      "\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mA dog riding a skateboard, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by artgerm and greg rutkowski and alphonse mucha\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? Yes\n",
      "Action: StableDiffusion\n",
      "Action Input: A dog riding a skateboard, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by artgerm and greg rutkowski and alphonse mucha\u001b[0m\n",
      "Job Status: Status.STARTING eta: None\n",
      "\n",
      "Job Status: Status.PROCESSING eta: None\n",
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3m/Users/harrisonchase/workplace/langchain/docs/modules/agents/tools/integrations/2e280ce4-4974-4420-8680-450825c31601/tmpfmiz2g1c.jpg\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? Yes\n",
      "Action: ImageCaptioner\n",
      "Action Input: /Users/harrisonchase/workplace/langchain/docs/modules/agents/tools/integrations/2e280ce4-4974-4420-8680-450825c31601/tmpfmiz2g1c.jpg\u001b[0m\n",
      "Job Status: Status.STARTING eta: None\n",
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3ma painting of a dog sitting on a skateboard\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? Yes\n",
      "Action: TextToVideo\n",
      "Action Input: a painting of a dog sitting on a skateboard\u001b[0m\n",
      "Job Status: Status.STARTING eta: None\n",
      "Due to heavy traffic on this app, the prediction will take approximately 73 seconds.For faster predictions without waiting in queue, you may duplicate the space using: Client.duplicate(damo-vilab/modelscope-text-to-video-synthesis)\n",
      "\n",
      "Job Status: Status.IN_QUEUE eta: 73.89824726581574\n",
      "Due to heavy traffic on this app, the prediction will take approximately 42 seconds.For faster predictions without waiting in queue, you may duplicate the space using: Client.duplicate(damo-vilab/modelscope-text-to-video-synthesis)\n",
      "\n",
      "Job Status: Status.IN_QUEUE eta: 42.49370198879602\n",
      "\n",
      "Job Status: Status.IN_QUEUE eta: 21.314297944849187\n",
      "\n",
      "Observation: \u001b[31;1m\u001b[1;3m/var/folders/bm/ylzhm36n075cslb9fvvbgq640000gn/T/tmp5snj_nmzf20_cb3m.mp4\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Do I need to use a tool? No\n",
      "AI: Here is a video of a painting of a dog sitting on a skateboard.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from gradio_tools.tools import (\n",
    "    ImageCaptioningTool,\n",
    "    StableDiffusionPromptGeneratorTool,\n",
    "    StableDiffusionTool,\n",
    "    TextToVideoTool,\n",
    ")\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "tools = [\n",
    "    StableDiffusionTool().langchain,\n",
    "    ImageCaptioningTool().langchain,\n",
    "    StableDiffusionPromptGeneratorTool().langchain,\n",
    "    TextToVideoTool().langchain,\n",
    "]\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, memory=memory, agent=\"conversational-react-description\", verbose=True\n",
    ")\n",
    "output = agent.run(\n",
    "    input=(\n",
    "        \"Please create a photo of a dog riding a skateboard \"\n",
    "        \"but improve my prompt prior to using an image generator.\"\n",
    "        \"Please caption the generated image and create a video for it using the improved prompt.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67642c82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}