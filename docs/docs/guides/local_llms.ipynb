{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8982428",
   "metadata": {},
   "source": [
    "# Run LLMs locally | LLMをローカルで実行する\n",
    "\n",
    "## Use case | 使用例\n",
    "\n",
    "[PrivateGPT](https://github.com/imartinez/privateGPT)、[llama.cpp](https://github.com/ggerganov/llama.cpp)、[GPT4All](https://github.com/nomic-ai/gpt4all)といったプロジェクトの人気が、LLM（Large Language Models）をローカル（自分のデバイス上）で実行する需要を強調しています。\n",
    "\n",
    "> The popularity of projects like [PrivateGPT](https://github.com/imartinez/privateGPT), [llama.cpp](https://github.com/ggerganov/llama.cpp), and [GPT4All](https://github.com/nomic-ai/gpt4all) underscore the demand to run LLMs locally (on your own device).\n",
    "\n",
    "これには少なくとも2つの重要な利点があります：\n",
    "\n",
    "> This has at least two important benefits:\n",
    "\n",
    "1. `プライバシー`：お客様のデータは第三者に送信されず、商用サービスの利用規約の対象にもなりません\n",
    "\n",
    "   > `Privacy`: Your data is not sent to a third party, and it is not subject to the terms of service of a commercial service\n",
    "\n",
    "2. `コスト`：推論にかかる費用は発生しないため、トークンを多用するアプリケーション（例えば、[長時間実行されるシミュレーション](https://twitter.com/RLanceMartin/status/1691097659262820352?s=20)や要約など）にとって重要です。\n",
    "\n",
    "   > `Cost`: There is no inference fee, which is important for token-intensive applications (e.g., [long-running simulations](https://twitter.com/RLanceMartin/status/1691097659262820352?s=20), summarization)\n",
    "\n",
    "\n",
    "## Overview | 概要\n",
    "\n",
    "LLMをローカルで実行するには、いくつかの要件があります：\n",
    "\n",
    "> Running an LLM locally requires a few things:\n",
    "\n",
    "1. `Open-source LLM`：自由に変更や共有が可能なオープンソースのLLM\n",
    "\n",
    "   > `Open-source LLM`: An open-source LLM that can be freely modified and shared\n",
    "\n",
    "2. `Inference`：このLLMを許容可能な遅延でお使いのデバイス上で実行する能力\n",
    "\n",
    "   > `Inference`: Ability to run this LLM on your device w/ acceptable latency\n",
    "\n",
    "\n",
    "### Open-source LLMs | オープンソースの大規模言語モデル\n",
    "\n",
    "ユーザーは、急速に成長している[オープンソースのLLMs](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better)のセットに今すぐアクセスできるようになりました。\n",
    "\n",
    "> Users can now gain access to a rapidly growing set of [open-source LLMs](https://cameronrwolfe.substack.com/p/the-history-of-open-source-llms-better).\n",
    "\n",
    "これらのLLMは、少なくとも2つの次元にわたって評価することができます（図を参照）：\n",
    "\n",
    "> These LLMs can be assessed across at least two dimensions (see figure):\n",
    "\n",
    "1. `Base model`：ベースモデルとは何ですか？また、どのように訓練されましたか？\n",
    "\n",
    "   > `Base model`: What is the base-model and how was it trained?\n",
    "\n",
    "2. `Fine-tuning approach`：ベースモデルはファインチューニングされましたか？もしファインチューニングされた場合、どの[指示セット](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms#%C2%A7alpaca-an-instruction-following-llama-model)が使用されましたか？\n",
    "\n",
    "   > `Fine-tuning approach`: Was the base-model fine-tuned and, if so, what [set of instructions](https://cameronrwolfe.substack.com/p/beyond-llama-the-power-of-open-llms#%C2%A7alpaca-an-instruction-following-llama-model) was used?\n",
    "\n",
    "\n",
    "![Image description](../../static/img/OSS_LLM_overview.png)\n",
    "\n",
    "これらのモデルの相対的なパフォーマンスは、以下を含む複数のリーダーボードを使用して評価することができます：\n",
    "\n",
    "> The relative performance of these models can be assessed using several leaderboards, including:\n",
    "\n",
    "1. [LmSys](https://chat.lmsys.org/?arena)\n",
    "\n",
    "   > [LmSys](https://chat.lmsys.org/?arena)\n",
    "\n",
    "2. [GPT4All](https://gpt4all.io/index.html)\n",
    "\n",
    "   > [GPT4All](https://gpt4all.io/index.html)\n",
    "\n",
    "3. [HuggingFace](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\n",
    "\n",
    "   > [HuggingFace](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\n",
    "\n",
    "\n",
    "### Inference | 推論\n",
    "\n",
    "このためにいくつかのフレームワークが登場し、様々なデバイス上でオープンソースのLLMの推論をサポートしています：\n",
    "\n",
    "> A few frameworks for this have emerged to support inference of open-source LLMs on various devices:\n",
    "\n",
    "1. `llama.cpp`（https://github.com/ggerganov/llama.cpp）: [重みの最適化／量子化](https://finbarr.ca/how-is-llama-cpp-possible/)を用いたllama推論コードのC++実装\n",
    "\n",
    "   > [`llama.cpp`](https://github.com/ggerganov/llama.cpp): C++ implementation of llama inference code with [weight optimization / quantization](https://finbarr.ca/how-is-llama-cpp-possible/)\n",
    "\n",
    "2. `gpt4all`：推論のための最適化されたCバックエンド\n",
    "\n",
    "   > [`gpt4all`](https://docs.gpt4all.io/index.html): Optimized C backend for inference\n",
    "\n",
    "3. `Ollama`（<https://ollama.ai/>）: モデルの重みと環境をアプリにパッケージ化して、デバイス上でLLMを提供する\n",
    "\n",
    "   > [`Ollama`](https://ollama.ai/): Bundles model weights and environment into an app that runs on device and serves the LLM\n",
    "\n",
    "\n",
    "一般的に、これらのフレームワークはいくつかのことを行います：\n",
    "\n",
    "> In general, these frameworks will do a few things:\n",
    "\n",
    "1. `Quantization`：生のモデルの重みのメモリ使用量を削減する\n",
    "\n",
    "   > `Quantization`: Reduce the memory footprint of the raw model weights\n",
    "\n",
    "2. `推論のための効率的な実装`：消費者向けハードウェア（例えば、CPUやラップトップのGPU）での推論をサポートします\n",
    "\n",
    "   > `Efficient implementation for inference`: Support inference on consumer hardware (e.g., CPU or laptop GPU)\n",
    "\n",
    "\n",
    "特に、量子化の重要性については、[この素晴らしい記事](https://finbarr.ca/how-is-llama-cpp-possible/)をご覧ください。\n",
    "\n",
    "> In particular, see [this excellent post](https://finbarr.ca/how-is-llama-cpp-possible/) on the importance of quantization.\n",
    "\n",
    "![Image description](../../static/img/llama-memory-weights.png)\n",
    "\n",
    "精度を下げることで、LLMをメモリに格納するために必要なメモリ量を大幅に削減します。\n",
    "\n",
    "> With less precision, we radically decrease the memory needed to store the LLM in memory.\n",
    "\n",
    "さらに、GPUメモリの帯域幅の重要性を[このシート](https://docs.google.com/spreadsheets/d/1OehfHHNSn66BP2h3Bxp2NJTVX97icU0GmCXF6pK23H8/edit#gid=0)で確認できます！\n",
    "\n",
    "> In addition, we can see the importance of GPU memory bandwidth [sheet](https://docs.google.com/spreadsheets/d/1OehfHHNSn66BP2h3Bxp2NJTVX97icU0GmCXF6pK23H8/edit#gid=0)!\n",
    "\n",
    "Mac M2 Maxは、より大きなGPUメモリ帯域幅のおかげで、推論処理においてM1よりも5〜6倍高速です。\n",
    "\n",
    "> A Mac M2 Max is 5-6x faster than a M1 for inference due to the larger GPU memory bandwidth.\n",
    "\n",
    "![Image description](../../static/img/llama_t_put.png)\n",
    "\n",
    "## Quickstart | クイックスタート\n",
    "\n",
    "`Ollama`は、macOS上で推論を簡単に実行する方法の一つです。\n",
    "\n",
    "> [`Ollama`](https://ollama.ai/) is one way to easily run inference on macOS.\n",
    "\n",
    "[こちら](https://github.com/jmorganca/ollama?tab=readme-ov-file#ollama)の指示には詳細が記載されており、私たちはそれを要約します：\n",
    "\n",
    "> The instructions [here](https://github.com/jmorganca/ollama?tab=readme-ov-file#ollama) provide details, which we summarize:\n",
    "\n",
    "* [アプリをダウンロードして実行する](https://ollama.ai/download)\n",
    "\n",
    "  > [Download and run](https://ollama.ai/download) the app\n",
    "\n",
    "* コマンドラインから、この[オプションのリスト](https://github.com/jmorganca/ollama)にあるモデルを取得してください。たとえば、`ollama pull llama2`のようにします。\n",
    "\n",
    "  > From command line, fetch a model from this [list of options](https://github.com/jmorganca/ollama): e.g., `ollama pull llama2`\n",
    "\n",
    "* アプリが実行中の場合、すべてのモデルは自動的に`localhost:11434`でサーブされます\n",
    "\n",
    "  > When the app is running, all models are automatically served on `localhost:11434`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86178adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first man on the moon was Neil Armstrong, who landed on the moon on July 20, 1969 as part of the Apollo 11 mission. obviously.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\")\n",
    "llm(\"The first man on the moon was ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ab645",
   "metadata": {},
   "source": [
    "トークンが生成されている間、それらをストリームします。\n",
    "\n",
    "> Stream tokens as they are being generated.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9cd83603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. февруари 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon's surface, famously declaring \"That's one small step for man, one giant leap for mankind\" as he took his first steps. He was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the moon during the mission."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. февруари 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind\" as he took his first steps. He was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the moon during the mission.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"llama2\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "llm(\"The first man on the moon was ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb27414",
   "metadata": {},
   "source": [
    "## Environment | 環境\n",
    "\n",
    "ローカルでモデルを実行する際、推論速度は課題となります（上記参照）。\n",
    "\n",
    "> Inference speed is a challenge when running models locally (see above).\n",
    "\n",
    "レイテンシーを最小限に抑えるためには、多くの消費者向けノートパソコン（例えば、[Appleのデバイス](https://www.apple.com/newsroom/2022/06/apple-unveils-m2-with-breakthrough-performance-and-capabilities/)）に搭載されているGPU上でモデルをローカルに実行することが望ましいです。\n",
    "\n",
    "> To minimize latency, it is desirable to run models locally on GPU, which ships with many consumer laptops [e.g., Apple devices](https://www.apple.com/newsroom/2022/06/apple-unveils-m2-with-breakthrough-performance-and-capabilities/).\n",
    "\n",
    "そして、GPUを使用していても、上記のように利用可能なGPUメモリの帯域幅は重要です。\n",
    "\n",
    "> And even with GPU, the available GPU memory bandwidth (as noted above) is important.\n",
    "\n",
    "### Running Apple silicon GPU | AppleシリコンGPUを搭載しています\n",
    "\n",
    "`Ollama`はAppleデバイスのGPUを自動的に利用します。\n",
    "\n",
    "> `Ollama` will automatically utilize the GPU on Apple devices.\n",
    "\n",
    "他のフレームワークでは、Apple GPUを利用するためにユーザーが環境を設定する必要があります。\n",
    "\n",
    "> Other frameworks require the user to set up the environment to utilize the Apple GPU.\n",
    "\n",
    "例えば、`llama.cpp` のPythonバインディングは、[Metal](https://developer.apple.com/metal/)を通じてGPUを使用するように設定できます。\n",
    "\n",
    "> For example, `llama.cpp` python bindings can be configured to use the GPU via [Metal](https://developer.apple.com/metal/).\n",
    "\n",
    "Metalは、Appleによって作られたグラフィックスとコンピュートのAPIで、GPUにほぼ直接アクセスする機能を提供します。\n",
    "\n",
    "> Metal is a graphics and compute API created by Apple providing near-direct access to the GPU.\n",
    "\n",
    "この機能を有効にするための[`llama.cpp`](docs/integrations/llms/llamacpp)のセットアップは[こちら](https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md)をご覧ください。\n",
    "\n",
    "> See the [`llama.cpp`](docs/integrations/llms/llamacpp) setup [here](https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md) to enable this.\n",
    "\n",
    "特に、condaがあなたが作成した正しい仮想環境（`miniforge3`）を使用していることを確認してください。\n",
    "\n",
    "> In particular, ensure that conda is using the correct virtual environment that you created (`miniforge3`).\n",
    "\n",
    "例えば、私の場合：\n",
    "\n",
    "> E.g., for me:\n",
    "\n",
    "```\n",
    "conda activate /Users/rlm/miniforge3/envs/llama\n",
    "```\n",
    "\n",
    "上記が確認できたら、次に進みます：\n",
    "\n",
    "> With the above confirmed, then:\n",
    "\n",
    "```\n",
    "CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c382e79a",
   "metadata": {},
   "source": [
    "## LLMs | LLMs（Large Language Models）\n",
    "\n",
    "量子化されたモデルの重みにアクセスする方法は様々あります。\n",
    "\n",
    "> There are various ways to gain access to quantized model weights.\n",
    "\n",
    "1. `HuggingFace` - 多くの量子化モデルがダウンロード可能で、[`llama.cpp`](https://github.com/ggerganov/llama.cpp)などのフレームワークで実行できます\n",
    "\n",
    "   > [`HuggingFace`](https://huggingface.co/TheBloke) - Many quantized model are available for download and can be run with framework such as [`llama.cpp`](https://github.com/ggerganov/llama.cpp)\n",
    "\n",
    "2. `gpt4all` - このモデルエクスプローラーは、ダウンロード可能な量子化モデルとそれに関連するメトリクスのリーダーボードを提供しています\n",
    "\n",
    "   > [`gpt4all`](https://gpt4all.io/index.html) - The model explorer offers a leaderboard of metrics and associated quantized models available for download\n",
    "\n",
    "3. `Ollama`（https://github.com/jmorganca/ollama）- 複数のモデルには、`pull`を通じて直接アクセスできます\n",
    "\n",
    "   > [`Ollama`](https://github.com/jmorganca/ollama) - Several models can be accessed directly via `pull`\n",
    "\n",
    "\n",
    "### Ollama | オラマ\n",
    "\n",
    "[Ollama](https://github.com/jmorganca/ollama)を使用して、`ollama pull <model family>:<tag>`コマンドでモデルを取得します：\n",
    "\n",
    "> With [Ollama](https://github.com/jmorganca/ollama), fetch a model via `ollama pull <model family>:<tag>`:\n",
    "\n",
    "* 例えば、Llama-7bの場合、`ollama pull llama2`を実行すると、モデルの最も基本的なバージョン（例えば、最小のパラメータ数と4ビット量子化）がダウンロードされます。\n",
    "\n",
    "  > E.g., for Llama-7b: `ollama pull llama2` will download the most basic version of the model (e.g., smallest # parameters and 4 bit quantization)\n",
    "\n",
    "* [モデルリスト](https://github.com/jmorganca/ollama?tab=readme-ov-file#model-library)から特定のバージョンを指定することもできます。たとえば、`ollama pull llama2:13b`のように指定できます。\n",
    "\n",
    "  > We can also specify a particular version from the [model list](https://github.com/jmorganca/ollama?tab=readme-ov-file#model-library), e.g., `ollama pull llama2:13b`\n",
    "\n",
    "* [APIリファレンスページ](https://api.python.langchain.com/en/latest/llms/langchain.llms.ollama.Ollama.html)で、パラメータの完全なセットをご覧ください。\n",
    "\n",
    "  > See the full set of parameters on the [API reference page](https://api.python.langchain.com/en/latest/llms/langchain.llms.ollama.Ollama.html)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ecd2f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sure! Here\\'s the answer, broken down step by step:\\n\\nThe first man on the moon was... Neil Armstrong.\\n\\nHere\\'s how I arrived at that answer:\\n\\n1. The first manned mission to land on the moon was Apollo 11.\\n2. The mission included three astronauts: Neil Armstrong, Edwin \"Buzz\" Aldrin, and Michael Collins.\\n3. Neil Armstrong was the mission commander and the first person to set foot on the moon.\\n4. On July 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind.\"\\n\\nSo, the first man on the moon was Neil Armstrong!'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2:13b\")\n",
    "llm(\"The first man on the moon was ... think step by step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c8c0d1",
   "metadata": {},
   "source": [
    "### Llama.cpp | Llama.cpp\n",
    "\n",
    "Llama.cppは、[幅広いモデルセット](https://github.com/ggerganov/llama.cpp)に対応しています。\n",
    "\n",
    "> Llama.cpp is compatible with a [broad set of models](https://github.com/ggerganov/llama.cpp).\n",
    "\n",
    "例えば、以下では、[HuggingFace](https://huggingface.co/TheBloke/Llama-2-13B-GGML/tree/main)からダウンロードした4ビット量子化された`llama2-13b`で推論を実行しています。\n",
    "\n",
    "> For example, below we run inference on `llama2-13b` with 4 bit quantization downloaded from [HuggingFace](https://huggingface.co/TheBloke/Llama-2-13B-GGML/tree/main).\n",
    "\n",
    "上記の通り、パラメーターの完全なセットについては、[APIリファレンス](https://api.python.langchain.com/en/latest/llms/langchain.llms.llamacpp.LlamaCpp.html?highlight=llamacpp#langchain.llms.llamacpp.LlamaCpp)をご覧ください。\n",
    "\n",
    "> As noted above, see the [API reference](https://api.python.langchain.com/en/latest/llms/langchain.llms.llamacpp.LlamaCpp.html?highlight=llamacpp#langchain.llms.llamacpp.LlamaCpp) for the full set of parameters.\n",
    "\n",
    "[llama.cppのドキュメント](https://python.langchain.com/docs/integrations/llms/llamacpp)には、コメントする価値のあるものがいくつかあります：\n",
    "\n",
    "> From the [llama.cpp docs](https://python.langchain.com/docs/integrations/llms/llamacpp), a few are worth commenting on:\n",
    "\n",
    "`n_gpu_layers`：GPUメモリにロードされるレイヤーの数\n",
    "\n",
    "> `n_gpu_layers`: number of layers to be loaded into GPU memory\n",
    "\n",
    "* 値: 1\n",
    "\n",
    "  > Value: 1\n",
    "\n",
    "* 意味：モデルのレイヤーは1つだけGPUメモリにロードされます（1つでしばしば十分です）。\n",
    "\n",
    "  > Meaning: Only one layer of the model will be loaded into GPU memory (1 is often sufficient).\n",
    "\n",
    "\n",
    "`n_batch`：モデルが並行して処理すべきトークンの数\n",
    "\n",
    "> `n_batch`: number of tokens the model should process in parallel\n",
    "\n",
    "* 値: n\\_batch\n",
    "\n",
    "  > Value: n\\_batch\n",
    "\n",
    "* 意味：1からn\\_ctx（この場合は2048に設定されている）の間の値を選ぶことをお勧めします\n",
    "\n",
    "  > Meaning: It's recommended to choose a value between 1 and n\\_ctx (which in this case is set to 2048)\n",
    "\n",
    "\n",
    "`n_ctx`: トークンのコンテキストウィンドウ\n",
    "\n",
    "> `n_ctx`: Token context window\n",
    "\n",
    "* 値: 2048\n",
    "\n",
    "  > Value: 2048\n",
    "\n",
    "* 意味：モデルは一度に2048トークンの範囲を考慮します\n",
    "\n",
    "  > Meaning: The model will consider a window of 2048 tokens at a time\n",
    "\n",
    "\n",
    "`f16_kv`：モデルがキー/バリューキャッシュに半精度を使用すべきかどうか\n",
    "\n",
    "> `f16_kv`: whether the model should use half-precision for the key/value cache\n",
    "\n",
    "* 値: True\n",
    "\n",
    "  > Value: True\n",
    "\n",
    "* 意味：モデルは半精度を使用しますが、これはメモリ効率がより良くなる可能性があります。ただし、MetalはTrueのみをサポートしています。\n",
    "\n",
    "  > Meaning: The model will use half-precision, which can be more memory efficient; Metal only supports True.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba38dc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%env CMAKE_ARGS=\"-DLLAMA_METAL=on\"\n",
    "%env FORCE_CMAKE=1\n",
    "%pip install -U llama-cpp-python --no-cache-dirclear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bf0c8-e989-4bcd-bcb7-4d7757e684f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f5168",
   "metadata": {},
   "source": [
    "コンソールログには、上記の手順でMetalが適切に有効化されたことを示す以下の表示が出ます：\n",
    "\n",
    "> The console log will show the below to indicate Metal was enabled properly from steps above:\n",
    "\n",
    "```\n",
    "ggml_metal_init: allocating\n",
    "ggml_metal_init: using MPS\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7890a077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and use logical reasoning to figure out who the first man on the moon was.\n",
      "\n",
      "Here are some clues:\n",
      "\n",
      "1. The first man on the moon was an American.\n",
      "2. He was part of the Apollo 11 mission.\n",
      "3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\n",
      "4. His last name is Armstrong.\n",
      "\n",
      "Now, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\n",
      "Therefore, the first man on the moon was Neil Armstrong!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  9623.21 ms\n",
      "llama_print_timings:      sample time =   143.77 ms /   203 runs   (    0.71 ms per token,  1412.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   485.94 ms /     7 tokens (   69.42 ms per token,    14.40 tokens per second)\n",
      "llama_print_timings:        eval time =  6385.16 ms /   202 runs   (   31.61 ms per token,    31.64 tokens per second)\n",
      "llama_print_timings:       total time =  7279.28 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" and use logical reasoning to figure out who the first man on the moon was.\\n\\nHere are some clues:\\n\\n1. The first man on the moon was an American.\\n2. He was part of the Apollo 11 mission.\\n3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\\n4. His last name is Armstrong.\\n\\nNow, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\\nTherefore, the first man on the moon was Neil Armstrong!\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"The first man on the moon was ... Let's think step by step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831ddf7c",
   "metadata": {},
   "source": [
    "### GPT4All | GPT4All\n",
    "\n",
    "[GPT4All](https://python.langchain.com/docs/integrations/llms/gpt4all)モデルエクスプローラーからダウンロードしたモデルの重みを使用することができます。\n",
    "\n",
    "> We can use model weights downloaded from [GPT4All](https://python.langchain.com/docs/integrations/llms/gpt4all) model explorer.\n",
    "\n",
    "上に示されているように、推論を実行し、[APIリファレンス](https://api.python.langchain.com/en/latest/llms/langchain.llms.gpt4all.GPT4All.html?highlight=gpt4all#langchain.llms.gpt4all.GPT4All)を使用して関心のあるパラメータを設定することができます。\n",
    "\n",
    "> Similar to what is shown above, we can run inference and use [the API reference](https://api.python.langchain.com/en/latest/llms/langchain.llms.gpt4all.GPT4All.html?highlight=gpt4all#langchain.llms.gpt4all.GPT4All) to set parameters of interest.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27baf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gpt4all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915ecd4c-8f6b-4de3-a787-b64cb7c682b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "\n",
    "llm = GPT4All(\n",
    "    model=\"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3d4526f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\".\\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon's surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"The first man on the moon was ... Let's think step by step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b84e543",
   "metadata": {},
   "source": [
    "## Prompts | プロンプト\n",
    "\n",
    "特定のプロンプトを使用することで、いくつかのLLMは恩恵を受けます。\n",
    "\n",
    "> Some LLMs will benefit from specific prompts.\n",
    "\n",
    "例えば、LLaMAは[特別なトークン](https://twitter.com/RLanceMartin/status/1681879318493003776?s=20)を使用します。\n",
    "\n",
    "> For example, LLaMA will use [special tokens](https://twitter.com/RLanceMartin/status/1681879318493003776?s=20).\n",
    "\n",
    "`ConditionalPromptSelector`を使用して、モデルタイプに基づいてプロンプトを設定することができます。\n",
    "\n",
    "> We can use `ConditionalPromptSelector` to set prompt based on the model type.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16759b7c-7903-4269-b7b4-f83b313d8091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our LLM\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",\n",
    "    n_gpu_layers=1,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66656084",
   "metadata": {},
   "source": [
    "モデルのバージョンに基づいて関連するプロンプトを設定してください。\n",
    "\n",
    "> Set the associated prompt based upon the model version.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8555f5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='<<SYS>> \\n You are an assistant tasked with improving Google search results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: \\n\\n {question} [/INST]', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.prompt_selector import ConditionalPromptSelector\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"<<SYS>> \\n You are an assistant tasked with improving Google search \\\n",
    "results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that \\\n",
    "are similar to this question. The output should be a numbered list of questions \\\n",
    "and each should have a question mark at the end: \\n\\n {question} [/INST]\"\"\",\n",
    ")\n",
    "\n",
    "DEFAULT_SEARCH_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an assistant tasked with improving Google search \\\n",
    "results. Generate THREE Google search queries that are similar to \\\n",
    "this question. The output should be a numbered list of questions and each \\\n",
    "should have a question mark at the end: {question}\"\"\",\n",
    ")\n",
    "\n",
    "QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(\n",
    "    default_prompt=DEFAULT_SEARCH_PROMPT,\n",
    "    conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)],\n",
    ")\n",
    "\n",
    "prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0aedfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here are three similar search queries with a question mark at the end:\n",
      "\n",
      "1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\n",
      "2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\n",
      "3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 14943.19 ms\n",
      "llama_print_timings:      sample time =    72.93 ms /   101 runs   (    0.72 ms per token,  1384.87 tokens per second)\n",
      "llama_print_timings: prompt eval time = 14942.95 ms /    93 tokens (  160.68 ms per token,     6.22 tokens per second)\n",
      "llama_print_timings:        eval time =  3430.85 ms /   100 runs   (   34.31 ms per token,    29.15 tokens per second)\n",
      "llama_print_timings:       total time = 18578.26 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  Sure! Here are three similar search queries with a question mark at the end:\\n\\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"What NFL team won the Super Bowl in the year that Justin Bieber was born?\"\n",
    "llm_chain.run({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0d37e7-f1d9-4848-bf2c-c22392ee141f",
   "metadata": {},
   "source": [
    "また、LangChain Prompt Hubを使用して、モデル固有のプロンプトを取得または保存することもできます。\n",
    "\n",
    "> We also can use the LangChain Prompt Hub to fetch and / or store prompts that are model specific.\n",
    "\n",
    "これはあなたの[LangSmith API key](https://docs.smith.langchain.com/)で動作します。\n",
    "\n",
    "> This will work with your [LangSmith API key](https://docs.smith.langchain.com/).\n",
    "\n",
    "例えば、LLaMA専用のトークンを使用したRAGのプロンプトは[こちら](https://smith.langchain.com/hub/rlm/rag-prompt-llama)です。\n",
    "\n",
    "> For example, [here](https://smith.langchain.com/hub/rlm/rag-prompt-llama) is a prompt for RAG with LLaMA-specific tokens.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba66260",
   "metadata": {},
   "source": [
    "## Use cases | 使用例\n",
    "\n",
    "上記のモデルのいずれかから作成された`llm`は、[多くのユースケース](docs/use_cases)で使用できます。\n",
    "\n",
    "> Given an `llm` created from one of the models above, you can use it for [many use cases](docs/use_cases).\n",
    "\n",
    "例えば、ローカルLLMを使用した[RAG](docs/use_cases/question_answering/local_retrieval_qa)のガイドはこちらです。\n",
    "\n",
    "> For example, here is a guide to [RAG](docs/use_cases/question_answering/local_retrieval_qa) with local LLMs.\n",
    "\n",
    "一般的に、ローカルLLMの使用事例は少なくとも2つの要因によって推進される可能性があります：\n",
    "\n",
    "> In general, use cases for local LLMs can be driven by at least two factors:\n",
    "\n",
    "* `プライバシー`：ユーザーが共有したくないプライベートデータ（例：日記など）\n",
    "\n",
    "  > `Privacy`: private data (e.g., journals, etc) that a user does not want to share\n",
    "\n",
    "* `Cost`：テキストの前処理（抽出・タグ付け）、要約、エージェントシミュレーションは、トークンの使用が集中するタスクです\n",
    "\n",
    "  > `Cost`: text preprocessing (extraction/tagging), summarization, and agent simulations are token-use-intensive tasks\n",
    "\n",
    "\n",
    "さらに、オープンソースのLLMを利用したファインチューニングについての概要は[こちら](https://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/)でご覧いただけます。\n",
    "\n",
    "> In addition, [here](https://blog.langchain.dev/using-langsmith-to-support-fine-tuning-of-open-source-llms/) is an overview on fine-tuning, which can utilize open-source LLMs.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}