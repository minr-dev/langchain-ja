# Conversation Summary | 会話の要約

次に、少し複雑なタイプのメモリーである `ConversationSummaryMemory` の使用方法について見ていきましょう。このタイプのメモリーは、時間をかけて会話の要約を作成する機能を持っています。これは、時間をかけて会話から情報を凝縮する際に役立ちます。
会話要約メモリーは、会話が進行するにつれてその要約を作成し、現在の要約をメモリーに保存します。このメモリーは、これまでの会話の要約をプロンプトやチェーンに挿入するために使用できます。このメモリーは、過去のメッセージ履歴をそのままプロンプトに保持するとトークンを多く消費してしまう長い会話に特に役立ちます。

> Now let's take a look at using a slightly more complex type of memory - `ConversationSummaryMemory`. This type of memory creates a summary of the conversation over time. This can be useful for condensing information from the conversation over time.
> Conversation summary memory summarizes the conversation as it happens and stores the current summary in memory. This memory can then be used to inject the summary of the conversation so far into a prompt/chain. This memory is most useful for longer conversations, where keeping the past message history in the prompt verbatim would take up too many tokens.

まずは、このタイプのメモリの基本的な機能を探ってみましょう。

> Let's first explore the basic functionality of this type of memory.

```python
from langchain.memory import ConversationSummaryMemory, ChatMessageHistory
from langchain.llms import OpenAI
```

```python
memory = ConversationSummaryMemory(llm=OpenAI(temperature=0))
memory.save_context({"input": "hi"}, {"output": "whats up"})
```

```python
memory.load_memory_variables({})
```

<CodeOutputBlock lang="python">

```
    {'history': '\nThe human greets the AI, to which the AI responds.'}
```

</CodeOutputBlock>

また、メッセージの履歴をリストとして取得することもできます（これはチャットモデルを使用している場合に便利です）。

> We can also get the history as a list of messages (this is useful if you are using this with a chat model).

```python
memory = ConversationSummaryMemory(llm=OpenAI(temperature=0), return_messages=True)
memory.save_context({"input": "hi"}, {"output": "whats up"})
```

```python
memory.load_memory_variables({})
```

<CodeOutputBlock lang="python">

```
    {'history': [SystemMessage(content='\nThe human greets the AI, to which the AI responds.', additional_kwargs={})]}
```

</CodeOutputBlock>

`predict_new_summary`メソッドも直接利用することができます。

> We can also utilize the `predict_new_summary` method directly.

```python
messages = memory.chat_memory.messages
previous_summary = ""
memory.predict_new_summary(messages, previous_summary)
```

<CodeOutputBlock lang="python">

```
    '\nThe human greets the AI, to which the AI responds.'
```

</CodeOutputBlock>

## Initializing with messages/existing summary | メッセージまたは既存の要約による初期化

このクラスの外部にメッセージがある場合、`ChatMessageHistory`を使ってクラスを簡単に初期化できます。読み込み中に、要約が計算されます。

> If you have messages outside this class, you can easily initialize the class with `ChatMessageHistory`. During loading, a summary will be calculated.

```python
history = ChatMessageHistory()
history.add_user_message("hi")
history.add_ai_message("hi there!")
```

```python
memory = ConversationSummaryMemory.from_messages(
    llm=OpenAI(temperature=0),
    chat_memory=history,
    return_messages=True
)
```

```python
memory.buffer
```

<CodeOutputBlock lang="python">

```
    '\nThe human greets the AI, to which the AI responds with a friendly greeting.'
```

</CodeOutputBlock>

オプションとして、以前に生成された要約を使用して初期化を高速化することができ、直接初期化することにより要約の再生成を回避することが可能です。

> Optionally you can speed up initialization using a previously generated summary, and avoid regenerating the summary by just initializing directly.

```python
memory = ConversationSummaryMemory(
    llm=OpenAI(temperature=0),
    buffer="The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.",
    chat_memory=history,
    return_messages=True
)
```

## Using in a chain | チェーンでの使用

この機能をチェーンで使用する例を見ていきましょう。プロンプトが表示されるように、再度`verbose=True`を設定します。

> Let's walk through an example of using this in a chain, again setting `verbose=True` so we can see the prompt.

```python
from langchain.llms import OpenAI
from langchain.chains import ConversationChain
llm = OpenAI(temperature=0)
conversation_with_summary = ConversationChain(
    llm=llm,
    memory=ConversationSummaryMemory(llm=OpenAI()),
    verbose=True
)
conversation_with_summary.predict(input="Hi, what's up?")
```

<CodeOutputBlock lang="python">

```


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Current conversation:

    Human: Hi, what's up?
    AI:

    > Finished chain.





    " Hi there! I'm doing great. I'm currently helping a customer with a technical issue. How about you?"
```

</CodeOutputBlock>

```python
conversation_with_summary.predict(input="Tell me more about it!")
```

<CodeOutputBlock lang="python">

```


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Current conversation:

    The human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue.
    Human: Tell me more about it!
    AI:

    > Finished chain.





    " Sure! The customer is having trouble with their computer not connecting to the internet. I'm helping them troubleshoot the issue and figure out what the problem is. So far, we've tried resetting the router and checking the network settings, but the issue still persists. We're currently looking into other possible solutions."
```

</CodeOutputBlock>

```python
conversation_with_summary.predict(input="Very cool -- what is the scope of the project?")
```

<CodeOutputBlock lang="python">

```


    > Entering new ConversationChain chain...
    Prompt after formatting:
    The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.

    Current conversation:

    The human greeted the AI and asked how it was doing. The AI replied that it was doing great and was currently helping a customer with a technical issue where their computer was not connecting to the internet. The AI was troubleshooting the issue and had already tried resetting the router and checking the network settings, but the issue still persisted and they were looking into other possible solutions.
    Human: Very cool -- what is the scope of the project?
    AI:

    > Finished chain.





    " The scope of the project is to troubleshoot the customer's computer issue and find a solution that will allow them to connect to the internet. We are currently exploring different possibilities and have already tried resetting the router and checking the network settings, but the issue still persists."
```

</CodeOutputBlock>
