{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titan Takeoff | タイタンの離陸\n",
    "\n",
    "> `TitanML`は、トレーニング、圧縮、および推論最適化プラットフォームを通じて、企業がより優れた、より小さく、より安価で、より高速なNLPモデルを構築し、デプロイするのを支援します。\n",
    ">\n",
    "> > `TitanML` helps businesses build and deploy better, smaller, cheaper, and faster NLP models through our training, compression, and inference optimization platform.\n",
    "\n",
    "> 私たちの推論サーバーである[Titan Takeoff](https://docs.titanml.co/docs/titan-takeoff/getting-started)は、単一のコマンドでお客様のハードウェア上にローカルでLLMsをデプロイすることを可能にします。Falcon、Llama 2、GPT-2、T5など、ほとんどの生成モデルアーキテクチャがサポートされています。\n",
    ">\n",
    "> > Our inference server, [Titan Takeoff](https://docs.titanml.co/docs/titan-takeoff/getting-started) enables deployment of LLMs locally on your hardware in a single command. Most generative model architectures are supported, such as Falcon, Llama 2, GPT2, T5 and many more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation | インストール\n",
    "\n",
    "Iris Takeoffを始めるために必要なのは、ローカルシステムにDockerとPythonがインストールされていることだけです。GPUサポート付きのサーバーを使用したい場合は、CUDAサポート付きのDockerをインストールする必要があります。\n",
    "\n",
    "> To get started with Iris Takeoff, all you need is to have docker and python installed on your local system. If you wish to use the server with gpu support, then you will need to install docker with cuda support.\n",
    "\n",
    "MacとWindowsユーザーの方は、dockerデーモンが実行中であることを確認してください！これを確認するには、ターミナルで`docker ps`を実行してください。デーモンを起動するには、Docker Desktopアプリを開いてください。\n",
    "\n",
    "> For Mac and Windows users, make sure you have the docker daemon running! You can check this by running docker ps in your terminal. To start the daemon, open the docker desktop app.\n",
    "\n",
    "takeoffサーバーを実行するために必要なIris CLIをインストールするには、以下のコマンドを実行してください：\n",
    "\n",
    "> Run the following command to install the Iris CLI that will enable you to run the takeoff server:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install titan-iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose a Model | モデルを選択\n",
    "\n",
    "Takeoffは、Falcon、MPT、Llamaなど、最も強力な生成テキストモデルの多くをサポートしています。詳細については、[サポートされているモデル](https://docs.titanml.co/docs/titan-takeoff/supported-models)をご覧ください。ご自身のモデルを使用する方法については、[カスタムモデル](https://docs.titanml.co/docs/titan-takeoff/Advanced/custom-models)のページをご覧ください。\n",
    "\n",
    "> Takeoff supports many of the most powerful generative text models, such as Falcon, MPT, and Llama. See the [supported models](https://docs.titanml.co/docs/titan-takeoff/supported-models) for more information. For information about using your own models, see the [custom models](https://docs.titanml.co/docs/titan-takeoff/Advanced/custom-models).\n",
    "\n",
    "このデモでは、falcon 7B instructモデルを使用していきます。これは指示に従って動作するように訓練された優れたオープンソースモデルで、CPU上でも容易に推論処理を行うことができるほど小さなサイズです。\n",
    "\n",
    "> Going forward in this demo we will be using the falcon 7B instruct model. This is a good open-source model that is trained to follow instructions, and is small enough to easily inference even on CPUs.\n",
    "\n",
    "## Taking off | 離陸\n",
    "\n",
    "モデルはHuggingFace上のモデルIDによって参照されます。Takeoffはデフォルトでポート8000を使用しますが、別のポートを使用するように設定することもできます。また、デバイスフラグでcudaを指定することにより、Nvidia GPUを使用するサポートもあります。\n",
    "\n",
    "> Models are referred to by their model id on HuggingFace. Takeoff uses port 8000 by default, but can be configured to use another port. There is also support to use a Nvidia GPU by specifying cuda for the device flag.\n",
    "\n",
    "takeoffサーバーを起動するには、以下のコマンドを実行してください：\n",
    "\n",
    "> To start the takeoff server, run:\n",
    "\n",
    "```shell\n",
    "iris takeoff --model tiiuae/falcon-7b-instruct --device cpu\n",
    "iris takeoff --model tiiuae/falcon-7b-instruct --device cuda # Nvidia GPU required\n",
    "iris takeoff --model tiiuae/falcon-7b-instruct --device cpu --port 5000 # run on port 5000 (default: 8000)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "その後、ログインページに移動します。そこでアカウントを作成して先に進む必要があります。ログインした後、サーバーが準備できているかどうかを確認するために画面に表示されたコマンドを実行してください。準備が整ったら、Takeoff統合の使用を開始できます。\n",
    "\n",
    "> You will then be directed to a login page, where you will need to create an account to proceed.\n",
    "> After logging in, run the command onscreen to check whether the server is ready. When it is ready, you can start using the Takeoff integration.\n",
    "\n",
    "サーバーをシャットダウンするには、以下のコマンドを実行してください。複数のサーバーが稼働している場合、どのTakeoffサーバーをシャットダウンするかのオプションが表示されます。\n",
    "\n",
    "> To shutdown the server, run the following command. You will be presented with options on which Takeoff server to shut down, in case you have multiple running servers.\n",
    "\n",
    "```shell\n",
    "iris takeoff --shutdown # shutdown the server\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing your model | モデルの推論処理\n",
    "\n",
    "LLMにアクセスするには、TitanTakeoff LLMラッパーを使用してください：\n",
    "\n",
    "> To access your LLM, use the TitanTakeoff LLM wrapper:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import TitanTakeoff\n",
    "\n",
    "llm = TitanTakeoff(\n",
    "    base_url=\"http://localhost:8000\", generate_max_length=128, temperature=1.0\n",
    ")\n",
    "\n",
    "prompt = \"What is the largest planet in the solar system?\"\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デフォルトではパラメータは必要ありませんが、Takeoffが実行されている希望するURLを指し示すbaseURLを指定でき、[生成パラメータ](https://docs.titanml.co/docs/titan-takeoff/Advanced/generation-parameters)を供給することも可能です。\n",
    "\n",
    "> No parameters are needed by default, but a baseURL that points to your desired URL where Takeoff is running can be specified and [generation parameters](https://docs.titanml.co/docs/titan-takeoff/Advanced/generation-parameters) can be supplied.\n",
    "\n",
    "### Streaming | ストリーミング\n",
    "\n",
    "ストリーミングは、ストリーミングフラグを通じてもサポートされています：\n",
    "\n",
    "> Streaming is also supported via the streaming flag:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "llm = TitanTakeoff(\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), streaming=True\n",
    ")\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with LLMChain | LLMChainとの統合\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = TitanTakeoff()\n",
    "\n",
    "template = \"What is the capital of {country}\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"country\"])\n",
    "\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "generated = llm_chain.run(country=\"Belgium\")\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}