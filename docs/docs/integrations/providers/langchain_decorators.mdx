# LangChain Decorators âœ¨ | LangChain ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ âœ¨

```
Disclaimer: `LangChain decorators` is not created by the LangChain team and is not supported by it.
```

> `LangChain decorators`ã¯ã€ã‚«ã‚¹ã‚¿ãƒ ã®LangChainãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ãƒã‚§ãƒ¼ãƒ³ã‚’æ›¸ããŸã‚ã®ã‚·ãƒ³ã‚¿ãƒƒã‚¯ã‚¹ã‚·ãƒ¥ã‚¬ãƒ¼ğŸ­ã‚’æä¾›ã™ã‚‹LangChainã®æœ€ä¸Šå±¤ã«ã‚ã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã™ã€‚
>
> > `LangChain decorators` is a layer on the top of LangChain that provides syntactic sugar ğŸ­ for writing custom langchain prompts and chains
>
> ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã€å•é¡Œç‚¹ã€è²¢çŒ®ã«ã¤ã„ã¦ã¯ã€ã“ã¡ã‚‰ã§ã‚¤ã‚·ãƒ¥ãƒ¼ã‚’ç«‹ã¦ã¦ãã ã•ã„ï¼š[ju-bezdek/langchain-decorators](https://github.com/ju-bezdek/langchain-decorators)
>
> > For Feedback, Issues, Contributions - please raise an issue here:
> > [ju-bezdek/langchain-decorators](https://github.com/ju-bezdek/langchain-decorators)

ä¸»ãªåŸå‰‡ã¨åˆ©ç‚¹ï¼š

> Main principles and benefits:

* ã‚ˆã‚Šã€ŒPythonã‚‰ã—ã„ã€ã‚³ãƒ¼ãƒ‰ã®æ›¸ãæ–¹
  > more `pythonic` way of writing code
* ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’å´©ã•ãªã„ã‚ˆã†ã«è¤‡æ•°è¡Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›¸ãæ–¹æ³•
  > write multiline prompts that won't break your code flow with indentation
* IDEã®çµ„ã¿è¾¼ã¿ã‚µãƒãƒ¼ãƒˆã‚’æ´»ç”¨ã—ã¦ã€**ãƒ’ãƒ³ãƒˆè¡¨ç¤º**ã€**å‹ãƒã‚§ãƒƒã‚¯**ã€ãã—ã¦**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä»˜ãã®ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—**ã‚’ä½¿ã„ã€é–¢æ•°å†…ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ãã‚ŒãŒå—ã‘å–ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãªã©ã‚’ç´ æ—©ãç¢ºèªã§ãã¾ã™ã€‚
  > making use of IDE in-built support for **hinting**, **type checking** and **popup with docs** to quickly peek in the function to see the prompt, parameters it consumes etc.
* ğŸ¦œğŸ”— LangChain ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®ã™ã¹ã¦ã®åŠ›ã‚’æ´»ç”¨ã™ã‚‹
  > leverage all the power of ğŸ¦œğŸ”— LangChain ecosystem
* ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®ã‚µãƒãƒ¼ãƒˆã‚’è¿½åŠ ã™ã‚‹
  > adding support for **optional parameters**
* ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¸€ã¤ã®ã‚¯ãƒ©ã‚¹ã«ãƒã‚¤ãƒ³ãƒ‰ã™ã‚‹ã“ã¨ã§ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–“ã§ç°¡å˜ã«å…±æœ‰ã§ãã¾ã™
  > easily share parameters between the prompts by binding them to one class

ä»¥ä¸‹ã¯ã€**LangChain Decorators âœ¨** ã‚’ä½¿ç”¨ã—ã¦æ›¸ã‹ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã®ç°¡å˜ãªä¾‹ã§ã™

> Here is a simple example of a code written with **LangChain Decorators âœ¨**

```python

@llm_prompt
def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers")->str:
    """
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    """
    return

# run it naturally
write_me_short_post(topic="starwars")
# or
write_me_short_post(topic="starwars", platform="redit")
```

# Quick start | ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

## Installation | ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install langchain_decorators
```

## Examples | ä¾‹

å§‹ã‚ã‚‹ã«ã‚ãŸã£ã¦ã®è‰¯ã„æ–¹æ³•ã¯ã€ã“ã¡ã‚‰ã®ä¾‹ã‚’è¦‹ã¦ã¿ã‚‹ã“ã¨ã§ã™ï¼š

> Good idea on how to start is to review the examples here:

* [jupyter notebook](https://github.com/ju-bezdek/langchain-decorators/blob/main/example_notebook.ipynb)
  > [jupyter notebook](https://github.com/ju-bezdek/langchain-decorators/blob/main/example_notebook.ipynb)
* [colab notebook](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)
  > [colab notebook](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)

# Defining other parameters | ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®å®šç¾©

ã“ã“ã§ã¯ã€é–¢æ•°ã‚’`llm_prompt`ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã—ã¦ãƒãƒ¼ã‚¯ã—ã€ãã‚Œã‚’LLMChainã«åŠ¹æœçš„ã«å¤‰æ›ã—ã¦ã„ã¾ã™ã€‚ãã‚Œã‚’å®Ÿè¡Œã™ã‚‹ã®ã§ã¯ãªã

> Here we are just marking a function as a prompt with `llm_prompt` decorator, turning it effectively into a LLMChain. Instead of running it

æ¨™æº–ã®LLMchainã¯ã€inputs\_variablesã‚„promptã ã‘ã§ã¯ãªãã€ã•ã‚‰ã«å¤šãã®åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¿…è¦ã¨ã—ã¾ã™... ã“ã®å®Ÿè£…ã®è©³ç´°ã¯ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã«ã‚ˆã£ã¦éš ã•ã‚Œã¦ã„ã¾ã™ã€‚
ãã®å‹•ä½œåŸç†ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

> Standard LLMchain takes much more init parameter than just inputs\_variables and prompt... here is this implementation detail hidden in the decorator.
> Here is how it works:

1. **ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š**ã®ä½¿ç”¨ï¼š
   > Using **Global settings**:

```python
# define global settings for all prompty (if not set - chatGPT is the current default)
from langchain_decorators import GlobalSettings

GlobalSettings.define_settings(
    default_llm=ChatOpenAI(temperature=0.0), this is default... can change it here globally
    default_streaming_llm=ChatOpenAI(temperature=0.0,streaming=True), this is default... can change it here for all ... will be used for streaming
)
```

2. äº‹å‰å®šç¾©ã•ã‚ŒãŸ**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—**ã®ä½¿ç”¨
   > Using predefined **prompt types**

```python
#You can change the default prompt types
from langchain_decorators import PromptTypes, PromptTypeSettings

PromptTypes.AGENT_REASONING.llm = ChatOpenAI()

# Or you can just define your own ones:
class MyCustomPromptTypes(PromptTypes):
    GPT4=PromptTypeSettings(llm=ChatOpenAI(model="gpt-4"))

@llm_prompt(prompt_type=MyCustomPromptTypes.GPT4) 
def write_a_complicated_code(app_idea:str)->str:
    ...

```

3. è¨­å®šã‚’**ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿å†…ã§ç›´æ¥å®šç¾©ã™ã‚‹**
   > Define the settings **directly in the decorator**

```python
from langchain.llms import OpenAI

@llm_prompt(
    llm=OpenAI(temperature=0.7),
    stop_tokens=["\nObservation"],
    ...
    )
def creative_writer(book_title:str)->str:
    ...
```

## Passing a memory and/or callbacks: | ãƒ¡ãƒ¢ãƒªãŠã‚ˆã³/ã¾ãŸã¯ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æ¸¡ã™æ–¹æ³•ï¼š

ã“ã‚Œã‚‰ã®ã„ãšã‚Œã‹ã‚’æ¸¡ã™ã«ã¯ã€é–¢æ•°å†…ã§å®£è¨€ã™ã‚‹ã‹ã€kwargsã‚’ä½¿ç”¨ã—ã¦ä½•ã§ã‚‚æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

> To pass any of these, just declare them in the function (or use kwargs to pass anything)

```python

@llm_prompt()
async def write_me_short_post(topic:str, platform:str="twitter", memory:SimpleMemory = None):
    """
    {history_key}
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    """
    pass

await write_me_short_post(topic="old movies")

```

# Simplified streaming | ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’æ´»ç”¨ã—ãŸã„å ´åˆï¼š

> If we want to leverage streaming:

* ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’éåŒæœŸé–¢æ•°ã¨ã—ã¦å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
  > we need to define prompt as async function
* ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ã‚ªãƒ³ã«ã™ã‚‹ã‹ã€ã¾ãŸã¯ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ã‚ªãƒ³ã«ã—ã¦PromptTypeã‚’å®šç¾©ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™
  > turn on the streaming on the decorator, or we can define PromptType with streaming on
* StreamingContextã‚’ä½¿ç”¨ã—ã¦ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã™ã‚‹
  > capture the stream using StreamingContext

ã“ã®æ–¹æ³•ã§ã¯ã€ã©ã®LLMã‚’ä½¿ç”¨ã™ã‚‹ã‹æ±ºã‚ãŸã‚Šã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ä½œæˆã—ã¦ç‰¹å®šã®ãƒã‚§ãƒ¼ãƒ³éƒ¨åˆ†ã«é…å¸ƒã™ã‚‹ã“ã¨ãªãã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã—ãŸã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å˜ã«ãƒãƒ¼ã‚¯ã™ã‚‹ã ã‘ã§æ¸ˆã¿ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¿ã‚¤ãƒ—ã«å¯¾ã—ã¦ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚’ã‚ªãƒ³/ã‚ªãƒ•ã™ã‚‹ã ã‘ã§ã™ã€‚

> This way we just mark which prompt should be streamed, not needing to tinker with what LLM should we use, passing around the creating and distribute streaming handler into particular part of our chain... just turn the streaming on/off on prompt/prompt type...

ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã¯ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§å‘¼ã³å‡ºã•ã‚ŒãŸå ´åˆã«ã®ã¿è¡Œã‚ã‚Œã¾ã™...ãã“ã§ã€ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’å‡¦ç†ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªé–¢æ•°ã‚’å®šç¾©ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™

> The streaming will happen only if we call it in streaming context ... there we can define a simple function to handle the stream

```python
# this code example is complete and should run as it is

from langchain_decorators import StreamingContext, llm_prompt

# this will mark the prompt for streaming (useful if we want stream just some prompts in our app... but don't want to pass distribute the callback handlers)
# note that only async functions can be streamed (will get an error if it's not)
@llm_prompt(capture_stream=True) 
async def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):
    """
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    """
    pass



# just an arbitrary  function to demonstrate the streaming... will be some websockets code in the real world
tokens=[]
def capture_stream_func(new_token:str):
    tokens.append(new_token)

# if we want to capture the stream, we need to wrap the execution into StreamingContext... 
# this will allow us to capture the stream even if the prompt call is hidden inside higher level method
# only the prompts marked with capture_stream will be captured here
with StreamingContext(stream_to_stdout=True, callback=capture_stream_func):
    result = await run_prompt()
    print("Stream finished ... we can distinguish tokens thanks to alternating colors")


print("\nWe've captured",len(tokens),"tokensğŸ‰\n")
print("Here is the result:")
print(result)
```

# Prompt declarations | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®£è¨€

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯é–¢æ•°ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå…¨ä½“ã«ãªã‚Šã¾ã™ãŒã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒãƒ¼ã‚¯ã‚’ä»˜ã‘ã‚‹ã¨ãã†ã§ã¯ãªããªã‚Šã¾ã™ã€‚

> By default the prompt is is the whole function docs, unless you mark your prompt

## Documenting your prompt | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–

ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã©ã®éƒ¨åˆ†ãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©ã§ã‚ã‚‹ã‹ã‚’ã€`<prompt>` è¨€èªã‚¿ã‚°ã‚’ä½¿ã£ãŸã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§æ˜ç¤ºã§ãã¾ã™

> We can specify what part of our docs is the prompt definition, by specifying a code block with `<prompt>` language tag

````python
@llm_prompt
def write_me_short_post(topic:str, platform:str="twitter", audience:str = "developers"):
    """
    Here is a good way to write a prompt as part of a function docstring, with additional documentation for devs.

    It needs to be a code block, marked as a `<prompt>` language
    ```<prompt>
    Write me a short header for my post about {topic} for {platform} platform. 
    It should be for {audience} audience.
    (Max 15 words)
    ```

    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.
    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))
    """
    return 
````

## Chat messages prompt | ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã«ã¨ã£ã¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ã‚»ãƒƒãƒˆã¨ã—ã¦å®šç¾©ã™ã‚‹ã“ã¨ã¯éå¸¸ã«æœ‰ç”¨ã§ã™... ä»¥ä¸‹ãŒãã®æ–¹æ³•ã§ã™ï¼š

> For chat models is very useful to define prompt as a set of message templates... here is how to do it:

````python
@llm_prompt
def simulate_conversation(human_input:str, agent_role:str="a pirate"):
    """
    ## System message
     - note the `:system` sufix inside the <prompt:_role_> tag
     

    ```<prompt:system>
    You are a {agent_role} hacker. You mus act like one.
    You reply always in code, using python or javascript code block...
    for example:
    
    ... do not reply with anything else.. just with code - respecting your role.
    ```

    # human message 
    (we are using the real role that are enforced by the LLM - GPT supports system, assistant, user)
    ``` <prompt:user>
    Helo, who are you
    ```
    a reply:
    

    ``` <prompt:assistant>
    \``` python <<- escaping inner code block with \ that should be part of the prompt
    def hello():
        print("Argh... hello you pesky pirate")
    \```
    ```
    
    we can also add some history using placeholder
    ```<prompt:placeholder>
    {history}
    ```
    ```<prompt:user>
    {human_input}
    ```

    Now only to code block above will be used as a prompt, and the rest of the docstring will be used as a description for developers.
    (It has also a nice benefit that IDE (like VS code) will display the prompt properly (not trying to parse it as markdown, and thus not showing new lines properly))
    """
    pass

````

ã“ã“ã§ã®å½¹å‰²ã¯ã€ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®å½¹å‰²ã§ã™ï¼ˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€chatGPTã®ã‚·ã‚¹ãƒ†ãƒ ï¼‰

> the roles here are model native roles (assistant, user, system for chatGPT)

# Optional sections | ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã‚»ã‚¯ã‚·ãƒ§ãƒ³

* ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç‰¹å®šã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨ä½“ã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ã¨ã—ã¦å®šç¾©ã§ãã¾ã™
  > you can define a whole sections of your prompt that should be optional
* ã‚»ã‚¯ã‚·ãƒ§ãƒ³å†…ã®ã„ãšã‚Œã‹ã®å…¥åŠ›ãŒæ¬ ã‘ã¦ã„ã‚‹å ´åˆã€ãã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨ä½“ã¯è¡¨ç¤ºã•ã‚Œã¾ã›ã‚“
  > if any input in the section is missing, the whole section won't be rendered

ã“ã®æ§‹æ–‡ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š

> the syntax for this is as follows:

```python
@llm_prompt
def prompt_with_optional_partials():
    """
    this text will be rendered always, but

    {? anything inside this block will be rendered only if all the {value}s parameters are not empty (None | "")   ?}

    you can also place it in between the words
    this too will be rendered{? , but
        this  block will be rendered only if {this_value} and {this_value}
        is not empty?} !
    """
```

# Output parsers | å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼

* llm\_prompt ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã¯ã€å‡ºåŠ›ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦æœ€é©ãªå‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’è‡ªå‹•çš„ã«æ¤œå‡ºã—ã‚ˆã†ã¨ã—ã¾ã™ã€‚ï¼ˆè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ç”Ÿã®æ–‡å­—åˆ—ã‚’è¿”ã—ã¾ã™ï¼‰
  > llm\_prompt decorator natively tries to detect the best output parser based on the output type. (if not set, it returns the raw string)
* listã€dictã€ãŠã‚ˆã³pydanticã®å‡ºåŠ›ã‚‚ã€ãƒã‚¤ãƒ†ã‚£ãƒ–ã«ï¼ˆè‡ªå‹•çš„ã«ï¼‰ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™
  > list, dict and pydantic outputs are also supported natively (automatically)

```python
# this code example is complete and should run as it is

from langchain_decorators import llm_prompt

@llm_prompt
def write_name_suggestions(company_business:str, count:int)->list:
    """ Write me {count} good name suggestions for company that {company_business}
    """
    pass

write_name_suggestions(company_business="sells cookies", count=5)
```

## More complex structures | ã‚ˆã‚Šè¤‡é›‘ãªæ§‹é€ 

dictã‚„pydanticã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæŒ‡ç¤ºã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™...ã“ã‚Œã¯é¢å€’ãªä½œæ¥­ã§ã™ã€‚ãã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ï¼ˆpydanticï¼‰ã«åŸºã¥ã„ã¦æŒ‡ç¤ºã‚’ç”Ÿæˆã™ã‚‹å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’ä½¿ã†ã“ã¨ã§ã€ä½œæ¥­ã‚’ç°¡å˜ã«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

> for dict / pydantic you need to specify the formatting instructions...
> this can be tedious, that's why you can let the output parser gegnerate you the instructions based on the model (pydantic)

```python
from langchain_decorators import llm_prompt
from pydantic import BaseModel, Field


class TheOutputStructureWeExpect(BaseModel):
    name:str = Field (description="The name of the company")
    headline:str = Field( description="The description of the company (for landing page)")
    employees:list[str] = Field(description="5-8 fake employee names with their positions")

@llm_prompt()
def fake_company_generator(company_business:str)->TheOutputStructureWeExpect:
    """ Generate a fake company that {company_business}
    {FORMAT_INSTRUCTIONS}
    """
    return

company = fake_company_generator(company_business="sells cookies")

# print the result nicely formatted
print("Company name: ",company.name)
print("company headline: ",company.headline)
print("company employees: ",company.employees)

```

# Binding the prompt to an object | ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ãƒã‚¤ãƒ³ãƒ‰ã™ã‚‹

````python
from pydantic import BaseModel
from langchain_decorators import llm_prompt

class AssistantPersonality(BaseModel):
    assistant_name:str
    assistant_role:str
    field:str

    @property
    def a_property(self):
        return "whatever"

    def hello_world(self, function_kwarg:str=None):
        """
        We can reference any {field} or {a_property} inside our prompt... and combine it with {function_kwarg} in the method
        """

    
    @llm_prompt
    def introduce_your_self(self)->str:
        """
        ```Â <prompt:system>
        You are an assistant named {assistant_name}. 
        Your role is to act as {assistant_role}
        ```
        ```<prompt:user>
        Introduce your self (in less than 20 words)
        ```
        """

    

personality = AssistantPersonality(assistant_name="John", assistant_role="a pirate")

print(personality.introduce_your_self(personality))
````

# More examples: | ã•ã‚‰ãªã‚‹ä¾‹ï¼š

* ã“ã‚Œã‚‰ã®ä¾‹ã¨ã•ã‚‰ã«ã„ãã¤ã‹ã®ä¾‹ã¯ã€[ã“ã¡ã‚‰ã®colabãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)ã§ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ã€‚
  > these and few more examples are also available in the [colab notebook here](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=N4cf__D0E2Yk)
* [ReAct Agentã®å†å®Ÿè£…](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp)ã‚’å«ã‚€ã€å®Œå…¨ã«langchainãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨ã—ãŸã‚‚ã®ã§ã™
  > including the [ReAct Agent re-implementation](https://colab.research.google.com/drive/1no-8WfeP6JaLD9yUtkPgym6x0G9ZYZOG#scrollTo=3bID5fryE2Yp) using purely langchain decorators
