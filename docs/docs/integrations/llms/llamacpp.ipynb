{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama.cpp | Llama.cpp",
    "",
    "[llama-cpp-python](https://github.com/abetlen/llama-cpp-python)は、[llama.cpp](https://github.com/ggerganov/llama.cpp)のためのPythonバインディングです。",
    "",
    "> [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) is a Python binding for [llama.cpp](https://github.com/ggerganov/llama.cpp).",
    "",
    "[多くのLLM](https://github.com/ggerganov/llama.cpp#description)モデルの推論をサポートしており、[Hugging Face](https://huggingface.co/TheBloke)でアクセスすることができます。",
    "",
    "> It supports inference for [many LLMs](https://github.com/ggerganov/llama.cpp#description) models, which can be accessed on [Hugging Face](https://huggingface.co/TheBloke).",
    "",
    "このノートブックでは、LangChain内で`llama-cpp-python`を実行する方法について説明します。",
    "",
    "> This notebook goes over how to run `llama-cpp-python` within LangChain.",
    "",
    "**注意: `llama-cpp-python` の新しいバージョンでは、GGUFモデルファイルが使用されます（詳細は[こちら](https://github.com/abetlen/llama-cpp-python/pull/633)をご覧ください）。**",
    "",
    "> **Note: new versions of `llama-cpp-python` use GGUF model files (see [here](https://github.com/abetlen/llama-cpp-python/pull/633)).**",
    "",
    "これは互換性のない変更です。",
    "",
    "> This is a breaking change.",
    "",
    "[llama.cpp](https://github.com/ggerganov/llama.cpp)で既存のGGMLモデルをGGUFに変換するには、以下を実行します：",
    "",
    "> To convert existing GGML models to GGUF you can run the following in [llama.cpp](https://github.com/ggerganov/llama.cpp):",
    "",
    "```",
    "python ./convert-llama-ggmlv3-to-gguf.py --eps 1e-5 --input models/openorca-platypus2-13b.ggmlv3.q4_0.bin --output models/openorca-platypus2-13b.gguf.q4_0.bin",
    "```",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation | インストール",
    "",
    "llama-cppパッケージのインストール方法には、いくつかのオプションがあります：",
    "",
    "> There are different options on how to install the llama-cpp package:",
    "",
    "* CPU使用率",
    "  > CPU usage",
    "* CPU + GPU（多くのBLASバックエンドのうちの1つを使用して）",
    "  > CPU + GPU (using one of many BLAS backends)",
    "* Metal GPU（Apple Siliconチップを搭載したMacOS）",
    "  > Metal GPU (MacOS with Apple Silicon Chip)",
    "",
    "### CPU only installation | CPUのみでのインストール",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation with OpenBLAS / cuBLAS / CLBlast | OpenBLAS / cuBLAS / CLBlast を使用したインストール",
    "",
    "`llama.cpp`は、より高速な処理のために複数のBLASバックエンドをサポートしています。`FORCE_CMAKE=1`環境変数を使用してcmakeの使用を強制し、希望するBLASバックエンド用のpipパッケージをインストールしてください（[ソース](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast)）。",
    "",
    "> `llama.cpp` supports multiple BLAS backends for faster processing. Use the `FORCE_CMAKE=1` environment variable to force the use of cmake and install the pip package for the desired BLAS backend ([source](https://github.com/abetlen/llama-cpp-python#installation-with-openblas--cublas--clblast)).",
    "",
    "cuBLASバックエンドを使用したインストール例：",
    "",
    "> Example installation with cuBLAS backend:",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**重要**: CPUのみのバージョンのパッケージを既にインストールしている場合は、最初から再インストールする必要があります。次のコマンドを参考にしてください：",
    "",
    "> **IMPORTANT**: If you have already installed the CPU only version of the package, you need to reinstall it from scratch. Consider the following command:",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation with Metal | Metalを使用したインストール",
    "",
    "`llama.cpp`はAppleシリコンを完全にサポートしており、ARM NEON、Accelerate、Metalフレームワークを通じて最適化されています。`FORCE_CMAKE=1`環境変数を設定してcmakeの使用を強制し、Metalサポートのpipパッケージをインストールできます（[ソース](https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md)）。",
    "",
    "> `llama.cpp` supports Apple silicon first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks. Use the `FORCE_CMAKE=1` environment variable to force the use of cmake and install the pip package for the Metal support ([source](https://github.com/abetlen/llama-cpp-python/blob/main/docs/install/macos.md)).",
    "",
    "Metalサポート付きのインストール例：",
    "",
    "> Example installation with Metal Support:",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**重要**: もしCPU専用バージョンのパッケージを既にインストールしている場合は、一から再インストールする必要があります。以下のコマンドを参考にしてください：",
    "",
    "> **IMPORTANT**: If you have already installed a cpu only version of the package, you need to reinstall it from scratch: consider the following command:",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation with Windows | Windowsにおけるインストール",
    "",
    "`llama-cpp-python` ライブラリはソースからコンパイルすることで安定してインストールできます。リポジトリ自体の指示にほとんど従えますが、Windows特有のいくつかの指示があり、それらは役立つことがあります。",
    "",
    "> It is stable to install the `llama-cpp-python` library by compiling from the source. You can follow most of the instructions in the repository itself but there are some windows specific instructions which might be useful.",
    "",
    "`llama-cpp-python`をインストールするための要件",
    "",
    "> Requirements to install the `llama-cpp-python`,",
    "",
    "* git",
    "  > git",
    "* python",
    "  > python",
    "* cmake",
    "  > cmake",
    "* Visual Studio Community（以下の設定でインストールしてください）",
    "  > Visual Studio Community (make sure you install this with the following settings)",
    "  * C++によるデスクトップ開発",
    "    > Desktop development with C++",
    "  * Python開発",
    "    > Python development",
    "  * C++を使用したLinux組み込み開発",
    "    > Linux embedded development with C++",
    "",
    "1. gitリポジトリを再帰的にクローンして、`llama.cpp`サブモジュールも同時に取得してください",
    "   > Clone git repository recursively to get `llama.cpp` submodule as well",
    "",
    "```",
    "git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.git",
    "```",
    "",
    "2. コマンドプロンプト（またはアナコンダプロンプトがインストールされている場合はそれ）を開き、インストールのために環境変数を設定してください。GPUがない場合は、以下の両方の変数を設定する必要があります。",
    "   > Open up command Prompt (or anaconda prompt if you have it installed), set up environment variables to install. Follow this if you do not have a GPU, you must set both of the following variables.",
    "",
    "```",
    "set FORCE_CMAKE=1",
    "set CMAKE_ARGS=-DLLAMA_CUBLAS=OFF",
    "```",
    "",
    "NVIDIA GPUをお持ちの場合、2番目の環境変数は無視しても構いません。",
    "",
    "> You can ignore the second environment variable if you have an NVIDIA GPU.",
    "",
    "#### Compiling and installing | コンパイルとインストール",
    "",
    "変数を設定したのと同じコマンドプロンプト（アナコンダプロンプト）で、`llama-cpp-python` ディレクトリに `cd` コマンドで移動し、以下のコマンドを実行してください。",
    "",
    "> In the same command prompt (anaconda prompt) you set the variables, you can `cd` into `llama-cpp-python` directory and run the following commands.",
    "",
    "```",
    "python setup.py clean",
    "python setup.py install",
    "```",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage | 使用方法",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[必要なモデルファイルをすべてインストールする](https://github.com/ggerganov/llama.cpp)ための指示に従っていることを確認してください。",
    "",
    "> Make sure you are following all instructions to [install all necessary model files](https://github.com/ggerganov/llama.cpp).",
    "",
    "LLMをローカルで実行するため、`API_TOKEN`は必要ありません。",
    "",
    "> You don't need an `API_TOKEN` as you will run the LLM locally.",
    "",
    "どのモデルが目的のマシンで使用するのに適しているかを理解することは価値があります。",
    "",
    "> It is worth understanding which models are suitable to be used on the desired machine.",
    "",
    "[TheBloke's](https://huggingface.co/TheBloke) の Hugging Face モデルには、「Provided files」セクションがあり、異なる量子化サイズと方法でモデルを実行するために必要なRAMが公開されています（例：[Llama2-7B-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF#provided-files)）。",
    "",
    "> [TheBloke's](https://huggingface.co/TheBloke) Hugging Face models have a `Provided files` section that exposes the RAM required to run models of different quantisation sizes and methods (eg: [Llama2-7B-Chat-GGUF](https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF#provided-files)).",
    "",
    "この[GitHub issue](https://github.com/facebookresearch/llama/issues/425)は、あなたのマシンに最適なモデルを見つけるのにも関連しています。",
    "",
    "> This [github issue](https://github.com/facebookresearch/llama/issues/425) is also relevant to find the right model for your machine.",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルに合ったテンプレートの使用を検討してください！正しいプロンプトテンプレートを得るために、Hugging Faceなどのモデルページをチェックしてください。",
    "",
    "> **Consider using a template that suits your model! Check the models page on Hugging Face etc. to get a correct prompting template.**",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU | CPU",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMA 2 7Bモデルを使用した例",
    "",
    "> Example using a LLaMA 2 7B model",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",\n",
    "    temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stephen Colbert:\n",
      "Yo, John, I heard you've been talkin' smack about me on your show.\n",
      "Let me tell you somethin', pal, I'm the king of late-night TV\n",
      "My satire is sharp as a razor, it cuts deeper than a knife\n",
      "While you're just a british bloke tryin' to be funny with your accent and your wit.\n",
      "John Oliver:\n",
      "Oh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\n",
      "My show is the one that people actually watch and listen to, not just for the laughs but for the facts.\n",
      "While you're busy talkin' trash, I'm out here bringing the truth to light.\n",
      "Stephen Colbert:\n",
      "Truth? Ha! You think your show is about truth? Please, it's all just a joke to you.\n",
      "You're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\n",
      "While I'm the one who's really makin' a difference, with my sat"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   358.60 ms\n",
      "llama_print_timings:      sample time =   172.55 ms /   256 runs   (    0.67 ms per token,  1483.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   613.36 ms /    16 tokens (   38.33 ms per token,    26.09 tokens per second)\n",
      "llama_print_timings:        eval time = 10151.17 ms /   255 runs   (   39.81 ms per token,    25.12 tokens per second)\n",
      "llama_print_timings:       total time = 11332.41 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nStephen Colbert:\\nYo, John, I heard you've been talkin' smack about me on your show.\\nLet me tell you somethin', pal, I'm the king of late-night TV\\nMy satire is sharp as a razor, it cuts deeper than a knife\\nWhile you're just a british bloke tryin' to be funny with your accent and your wit.\\nJohn Oliver:\\nOh Stephen, don't be ridiculous, you may have the ratings but I got the real talk.\\nMy show is the one that people actually watch and listen to, not just for the laughs but for the facts.\\nWhile you're busy talkin' trash, I'm out here bringing the truth to light.\\nStephen Colbert:\\nTruth? Ha! You think your show is about truth? Please, it's all just a joke to you.\\nYou're just a fancy-pants british guy tryin' to be funny with your news and your jokes.\\nWhile I'm the one who's really makin' a difference, with my sat\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Question: A rap battle between Stephen Colbert and John Oliver\n",
    "\"\"\"\n",
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLaMA v1モデルを使用した例",
    "",
    "> Example using a LLaMA v1 model",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./ggml-model-q4_0.bin\", callback_manager=callback_manager, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. First, find out when Justin Bieber was born.\n",
      "2. We know that Justin Bieber was born on March 1, 1994.\n",
      "3. Next, we need to look up when the Super Bowl was played in that year.\n",
      "4. The Super Bowl was played on January 28, 1995.\n",
      "5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   434.15 ms\n",
      "llama_print_timings:      sample time =    41.81 ms /   121 runs   (    0.35 ms per token)\n",
      "llama_print_timings: prompt eval time =  2523.78 ms /    48 tokens (   52.58 ms per token)\n",
      "llama_print_timings:        eval time = 23971.57 ms /   121 runs   (  198.11 ms per token)\n",
      "llama_print_timings:       total time = 28945.95 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n1. First, find out when Justin Bieber was born.\\n2. We know that Justin Bieber was born on March 1, 1994.\\n3. Next, we need to look up when the Super Bowl was played in that year.\\n4. The Super Bowl was played on January 28, 1995.\\n5. Finally, we can use this information to answer the question. The NFL team that won the Super Bowl in the year Justin Bieber was born is the San Francisco 49ers.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU | GPU",
    "",
    "BLASバックエンドでのインストールが正しく行われた場合、モデルのプロパティに`BLAS = 1`の表示がされます。",
    "",
    "> If the installation with BLAS backend was correct, you will see a `BLAS = 1` indicator in model properties.",
    "",
    "GPUを使用する際の最も重要なパラメーターの2つは以下の通りです：",
    "",
    "> Two of the most important parameters for use with GPU are:",
    "",
    "* `n_gpu_layers` - モデルの何層をGPUにオフロードするかを決定します。",
    "  > `n_gpu_layers` - determines how many layers of the model are offloaded to your GPU.",
    "* `n_batch` - 並列処理されるトークンの数。",
    "  > `n_batch` - how many tokens are processed in parallel.",
    "",
    "これらのパラメータを正しく設定することで、評価速度が劇的に向上します（詳細は[ラッパーコード](https://github.com/mmagnesium/langchain/blob/master/langchain/llms/llamacpp.py)をご覧ください）。",
    "",
    "> Setting these parameters correctly will dramatically improve the evaluation speed (see [wrapper code](https://github.com/mmagnesium/langchain/blob/master/langchain/llms/llamacpp.py) for more details).",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu_layers = 40  # Change this value based on your model and your GPU VRAM pool.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\n",
      "\n",
      "2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\n",
      "\n",
      "3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\n",
      "\n",
      "So, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   427.63 ms\n",
      "llama_print_timings:      sample time =   115.85 ms /   164 runs   (    0.71 ms per token,  1415.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   427.53 ms /    45 tokens (    9.50 ms per token,   105.26 tokens per second)\n",
      "llama_print_timings:        eval time =  4526.53 ms /   163 runs   (   27.77 ms per token,    36.01 tokens per second)\n",
      "llama_print_timings:       total time =  5293.77 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n1. Identify Justin Bieber's birth date: Justin Bieber was born on March 1, 1994.\\n\\n2. Find the Super Bowl winner of that year: The NFL season of 1993 with the Super Bowl being played in January or of 1994.\\n\\n3. Determine which team won the game: The Dallas Cowboys faced the Buffalo Bills in Super Bowl XXVII on January 31, 1993 (as the year is mis-labelled due to a error). The Dallas Cowboys won this matchup.\\n\\nSo, Justin Bieber was born when the Dallas Cowboys were the reigning NFL Super Bowl.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "llm_chain.run(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metal | メタル",
    "",
    "Metalを使ったインストールが正しく行われた場合、モデルのプロパティに`NEON = 1`というインジケーターが表示されます。",
    "",
    "> If the installation with Metal was correct, you will see a `NEON = 1` indicator in model properties.",
    "",
    "最も重要なGPUのパラメーターは2つあります：",
    "",
    "> Two of the most important GPU parameters are:",
    "",
    "* `n_gpu_layers` - モデルの何層をMetal GPUにオフロードするかを決定します。ほとんどのケースでは、`1`に設定するだけでMetal GPUでの処理に十分です",
    "  > `n_gpu_layers` - determines how many layers of the model are offloaded to your Metal GPU, in the most case, set it to `1` is enough for Metal",
    "* `n_batch` - 並行して処理されるトークンの数で、デフォルトは8です。必要に応じてより大きな数値に設定してください。",
    "  > `n_batch` - how many tokens are processed in parallel, default is 8, set to bigger number.",
    "* `f16_kv` - 何らかの理由で、Metalは`True`のみをサポートしており、それ以外の値を使用すると`Asserting on type 0",
    "  GGML_ASSERT: .../ggml-metal.m:706: false && \"not implemented\"`のようなエラーが発生します。",
    "  > `f16_kv` - for some reason, Metal only support `True`, otherwise you will get error such as `Asserting on type 0",
    "  > GGML_ASSERT: .../ggml-metal.m:706: false && \"not implemented\"`",
    "",
    "これらのパラメータを正しく設定することで、評価速度が劇的に向上します（詳細は[ラッパーコード](https://github.com/mmagnesium/langchain/blob/master/langchain/llms/llamacpp.py)をご覧ください）。",
    "",
    "> Setting these parameters correctly will dramatically improve the evaluation speed (see [wrapper code](https://github.com/mmagnesium/langchain/blob/master/langchain/llms/llamacpp.py) for more details).",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "コンソールログには、Metalが適切に有効にされたことを示す以下のログが表示されます。",
    "",
    "> The console log will show the following log to indicate Metal was enable properly.",
    "",
    "```",
    "ggml_metal_init: allocating",
    "ggml_metal_init: using MPS",
    "...",
    "```",
    "",
    "また、プロセスのGPU使用率を`Activity Monitor`で監視することで、`n_gpu_layers=1`を有効にした後、CPUの使用率が劇的に低下することを確認できます。",
    "",
    "> You also could check `Activity Monitor` by watching the GPU usage of the process, the CPU usage will drop dramatically after turn on `n_gpu_layers=1`.",
    "",
    "LLMへの最初の呼び出しでは、Metal GPUでのモデルコンパイルのため、パフォーマンスが遅くなることがあります。",
    "",
    "> For the first call to the LLM, the performance may be slow due to the model compilation in Metal GPU.",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammars | 文法",
    "",
    "[グラマー](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md)を使用して、モデルの出力を制約し、それに定義されたルールに基づいてトークンをサンプリングすることができます。",
    "",
    "> We can use [grammars](https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md) to constrain model outputs and sample tokens based on the rules defined in them.",
    "",
    "この概念を説明するために、以下の例で使用される[サンプル文法ファイル](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/llms/grammars)を掲載しています。",
    "",
    "> To demonstrate this concept, we've included [sample grammar files](https://github.com/langchain-ai/langchain/tree/master/libs/langchain/langchain/llms/grammars), that will be used in the examples below.",
    "",
    "gbnf文法ファイルを作成することは時間がかかる作業ですが、出力スキーマが重要なユースケースがある場合、役立つ2つのツールがあります：",
    "",
    "> Creating gbnf grammar files can be time-consuming, but if you have a use-case where output schemas are important, there are two tools that can help:",
    "",
    "* [TypeScriptインターフェース定義をgbnfファイルに変換するオンライン文法生成アプリ](https://grammar.intrinsiclabs.ai/)",
    "  > [Online grammar generator app](https://grammar.intrinsiclabs.ai/) that converts TypeScript interface definitions to gbnf file.",
    "* JSONスキーマをGBNFファイルに変換するための[Pythonスクリプト](https://github.com/ggerganov/llama.cpp/blob/master/examples/json-schema-to-grammar.py)です。例えば、`pydantic`オブジェクトを作成し、`.schema_json()`メソッドを使用してそのJSONスキーマを生成した後、このスクリプトを使用してそれをGBNFファイルに変換することができます。",
    "  > [Python script](https://github.com/ggerganov/llama.cpp/blob/master/examples/json-schema-to-grammar.py) for converting json schema to gbnf file. You can for example create `pydantic` object, generate its JSON schema using `.schema_json()` method, and then use this script to convert it to gbnf file.",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初の例では、JSONを生成するために指定された`json.gbnf`ファイルへのパスを指定してください：",
    "",
    "> In the first example, supply the path to the specified `json.gbnf` file in order to produce JSON:",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    "    grammar_path=\"/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/json.gbnf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"John Doe\",\n",
      "  \"age\": 34,\n",
      "  \"\": {\n",
      "    \"title\": \"Software Developer\",\n",
      "    \"company\": \"Google\"\n",
      "  },\n",
      "  \"interests\": [\n",
      "    \"Sports\",\n",
      "    \"Music\",\n",
      "    \"Cooking\"\n",
      "  ],\n",
      "  \"address\": {\n",
      "    \"street_number\": 123,\n",
      "    \"street_name\": \"Oak Street\",\n",
      "    \"city\": \"Mountain View\",\n",
      "    \"state\": \"California\",\n",
      "    \"postal_code\": 94040\n",
      "  }}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   357.51 ms\n",
      "llama_print_timings:      sample time =  1213.30 ms /   144 runs   (    8.43 ms per token,   118.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   356.78 ms /     9 tokens (   39.64 ms per token,    25.23 tokens per second)\n",
      "llama_print_timings:        eval time =  3947.16 ms /   143 runs   (   27.60 ms per token,    36.23 tokens per second)\n",
      "llama_print_timings:       total time =  5846.21 ms\n"
     ]
    }
   ],
   "source": [
    "%%capture captured --no-stdout\n",
    "result = llm(\"Describe a person in JSON format:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`list.gbnf`を指定することで、リストを返すことができます：",
    "",
    "> We can also supply `list.gbnf` to return a list:",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu_layers = 1\n",
    "n_batch = 512\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,\n",
    "    grammar_path=\"/Users/rlm/Desktop/Code/langchain-main/langchain/libs/langchain/langchain/llms/grammars/list.gbnf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The Catcher in the Rye\", \"Wuthering Heights\", \"Anna Karenina\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   322.34 ms\n",
      "llama_print_timings:      sample time =   232.60 ms /    26 runs   (    8.95 ms per token,   111.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   321.90 ms /    11 tokens (   29.26 ms per token,    34.17 tokens per second)\n",
      "llama_print_timings:        eval time =   680.82 ms /    25 runs   (   27.23 ms per token,    36.72 tokens per second)\n",
      "llama_print_timings:       total time =  1295.27 ms\n"
     ]
    }
   ],
   "source": [
    "%%capture captured --no-stdout\n",
    "result = llm(\"List of top-3 my favourite books:\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('langchain_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1d3a3c58a58885896c5459933a599607cdbb9917d7e1ad7516c8786c51f2dd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}