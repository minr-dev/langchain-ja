# Vectara | Vectara

> [Vectara](https://vectara.com/)は、開発者向けの信頼されたGenAIプラットフォームです。セマンティック検索やRAG（Retrieval Augmented Generation）のためのGenAIアプリケーションを構築するためのシンプルなAPIを提供しています。
>
> > [Vectara](https://vectara.com/) is the trusted GenAI platform for developers. It provides a simple API to build GenAI applications
> > for semantic search or RAG (Retreieval augmented generation).

**Vectaraの概要:**

> **Vectara Overview:**

* `Vectara`は、信頼できるGenAIアプリケーションを構築するための開発者優先のAPIプラットフォームです。
  > `Vectara` is developer-first API platform for building trusted GenAI applications.
* Vectaraを使用するには、まず[サインアップ](https://vectara.com/integrations/langchain)してアカウントを作成してください。その後、コーパスとインデックス作成および検索用のAPIキーを作成してください。
  > To use Vectara - first [sign up](https://vectara.com/integrations/langchain) and create an account. Then create a corpus and an API key for indexing and searching.
* Vectaraの[indexing API](https://docs.vectara.com/docs/indexing-apis/indexing)を使用して、ドキュメントをVectaraのインデックスに追加できます。
  > You can use Vectara's [indexing API](https://docs.vectara.com/docs/indexing-apis/indexing) to add documents into Vectara's index
* Vectaraの[Search API](https://docs.vectara.com/docs/search-apis/search)を使用すると、Vectaraのインデックス（ハイブリッド検索も暗黙のうちにサポートされています）に対してクエリを実行できます。
  > You can use Vectara's [Search API](https://docs.vectara.com/docs/search-apis/search) to query Vectara's index (which also supports Hybrid search implicitly).

## Installation and Setup | インストールとセットアップ

`Vectara`をLangChainで使用するには、特別なインストール手順は必要ありません。始めるには、[サインアップ](https://vectara.com/integrations/langchain)して、コーパスとAPIキーを作成するための[クイックスタート](https://docs.vectara.com/docs/quickstart)ガイドに従ってください。これらを入手したら、Vectaraベクターストアに引数として提供することも、環境変数として設定することもできます。

> To use `Vectara` with LangChain no special installation steps are required.
> To get started, [sign up](https://vectara.com/integrations/langchain) and follow our [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key.
> Once you have these, you can provide them as arguments to the Vectara vectorstore, or you can set them as environment variables.

* `VECTARA_CUSTOMER_ID`に`your_customer_id`を設定し、エクスポートします
  > export `VECTARA_CUSTOMER_ID`="your\_customer\_id"
* `VECTARA_CORPUS_ID`を`your_corpus_id`に設定し、エクスポートしてください。
  > export `VECTARA_CORPUS_ID`="your\_corpus\_id"
* `VECTARA_API_KEY`を`"your-vectara-api-key"`としてエクスポートしてください。
  > export `VECTARA_API_KEY`="your-vectara-api-key"

## Vectara as a Vector Store | Vectaraをベクトルストアとして

Vectaraプラットフォームには、セマンティック検索や例示選択に関わらず、ベクトルストアとして使用できるラッパーが存在します。

> There exists a wrapper around the Vectara platform, allowing you to use it as a vectorstore, whether for semantic search or example selection.

このベクトルストアをインポートするには：

> To import this vectorstore:

```python
from langchain.vectorstores import Vectara
```

Vectaraベクターストアのインスタンスを作成するには：

> To create an instance of the Vectara vectorstore:

```python
vectara = Vectara(
    vectara_customer_id=customer_id, 
    vectara_corpus_id=corpus_id, 
    vectara_api_key=api_key
)
```

customer\_id、corpus\_id、api\_keyはオプションであり、提供されない場合はそれぞれ環境変数`VECTARA_CUSTOMER_ID`、`VECTARA_CORPUS_ID`、`VECTARA_API_KEY`から読み込まれます。

> The customer\_id, corpus\_id and api\_key are optional, and if they are not supplied will be read from the environment variables `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY`, respectively.

ベクトルストアを手に入れたら、標準の `VectorStore` インターフェースに従って、`add_texts` や `add_documents` を使用してテキストやドキュメントを追加できます。例えば以下のようにします：

> After you have the vectorstore, you can `add_texts` or `add_documents` as per the standard `VectorStore` interface, for example:

```python
vectara.add_texts(["to be or not to be", "that is the question"])
```

Vectaraはファイルアップロードに対応しているため、ファイル（PDF、TXT、HTML、PPT、DOCなど）を直接アップロードする機能も追加しました。この方法を使用すると、ファイルは直接Vectaraのバックエンドにアップロードされ、そこで最適に処理され分割されますので、LangChainのドキュメントローダーや分割メカニズムを使用する必要がなくなります。

> Since Vectara supports file-upload, we also added the ability to upload files (PDF, TXT, HTML, PPT, DOC, etc) directly as file. When using this method, the file is uploaded directly to the Vectara backend, processed and chunked optimally there, so you don't have to use the LangChain document loader or chunking mechanism.

例として：

> As an example:

```python
vectara.add_files(["path/to/file1.pdf", "path/to/file2.pdf",...])
```

ベクトルストアをクエリするには、クエリ文字列を受け取って結果のリストを返す `similarity_search` メソッド（または `similarity_search_with_score`）を使用できます：

> To query the vectorstore, you can use the `similarity_search` method (or `similarity_search_with_score`), which takes a query string and returns a list of results:

```python
results = vectara.similarity_score("what is LangChain?")
```

結果は関連するドキュメントのリストと、各ドキュメントの関連性スコアとして返されます。

> The results are returned as a list of relevant documents, and a relevance score of each document.

この場合、デフォルトの検索パラメータを使用しましたが、`similarity_search`や`similarity_search_with_score`で以下の追加引数を指定することもできます：

> In this case, we used the default retrieval parameters, but you can also specify the following additional arguments in `similarity_search` or `similarity_search_with_score`:

* `k`：返すべき結果の数（デフォルトは5）
  > `k`: number of results to return (defaults to 5)
* `lambda_val`: ハイブリッド検索における[レキシカルマッチング](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching)の係数で、デフォルト値は0.025です
  > `lambda_val`: the [lexical matching](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) factor for hybrid search (defaults to 0.025)
* `filter`: 結果に適用する[フィルター](https://docs.vectara.com/docs/common-use-cases/filtering-by-metadata/filter-overview)（デフォルトはNone）
  > `filter`: a [filter](https://docs.vectara.com/docs/common-use-cases/filtering-by-metadata/filter-overview) to apply to the results (default None)
* `n_sentence_context`：結果を返す際に、実際の一致するセグメントの前後に含めるべき文の数です。デフォルトは2です。
  > `n_sentence_context`: number of sentences to include before/after the actual matching segment when returning results. This defaults to 2.
* `mmr_config`：クエリでMMRモードを指定するために使用できます。
  > `mmr_config`: can be used to specify MMR mode in the query.
  * `is_enabled`: TrueまたはFalse
    > `is_enabled`: True or False
  * `mmr_k`：MMR再ランキングに使用する結果の数
    > `mmr_k`: number of results to use for MMR reranking
  * `diversity_bias`: 0は多様性がないことを意味し、1は完全な多様性を意味します。これはMMR式のλ（ラムダ）パラメータで、その範囲は0から1です。
    > `diversity_bias`: 0 = no diversity, 1 = full diversity. This is the lambda parameter in the MMR formula and is in the range 0...1

## Vectara for Retrieval Augmented Generation (RAG) | 検索強化生成（RAG）のためのVectara

Vectaraは、生成的要約を含む完全なRAGパイプラインを提供しています。
このパイプラインを使用するには、`similarity_search`または`similarity_search_with_score`の`summary_config`引数を以下のように指定します：

> Vectara provides a full RAG pipeline, including generative summarization.
> To use this pipeline, you can specify the `summary_config` argument in `similarity_search` or `similarity_search_with_score` as follows:

* `summary_config`：RAGでLLMの要約をリクエストするために使用できます
  > `summary_config`: can be used to request an LLM summary in RAG
  * `is_enabled`: TrueまたはFalse
    > `is_enabled`: True or False
  * `max_results`: 要約生成に使用する結果の数
    > `max_results`: number of results to use for summary generation
  * `response_lang`：レスポンスの要約の言語をISO 639-2形式で指定します（例：'en'、'fr'、'de'など）
    > `response_lang`: language of the response summary, in ISO 639-2 format (e.g. 'en', 'fr', 'de', etc)

## Example Notebooks | サンプルノートブック

Vectaraの使用方法に関する詳細な例については、以下の例を参照してください：

> For a more detailed examples of using Vectara, see the following examples:

* このノートブックは、セマンティック検索のためのベクトルストアとしてVectaraを使用する方法を示しています。
  > [this notebook](/docs/integrations/vectorstores/vectara.html) shows how to use Vectara as a vectorstore for semantic search
* このノートブックは、LangchainとVectaraを使用してチャットボットを構築する方法を示しています
  > [this notebook](/docs/integrations/providers/vectara/vectara_chat.html) shows how to build a chatbot with Langchain and Vectara
* このノートブックは、生成的要約を含むVectara RAGパイプライン全体の使用方法を示しています
  > [this notebook](/docs/integrations/providers/vectara/vectara_summary.html) shows how to use the full Vectara RAG pipeline, including generative summarization
* このノートブックは、Vectaraを使用したセルフクエリ機能を示しています。
  > [this notebook](/docs/integrations/retrievers/self_query/vectara_self_query.html) shows the self-query capability with Vectara.
