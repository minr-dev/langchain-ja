# NIBittensor | NIBittensor

このページでは、LangChain内でBittensorLLM推論ランタイムを使用する方法について説明しています。それはインストールとセットアップ、そしてNIBittensorLLMの使用例という2つのパートに分かれています。

> This page covers how to use the BittensorLLM inference runtime within LangChain.
> It is broken into two parts: installation and setup, and then examples of NIBittensorLLM usage.

## Installation and Setup | インストールとセットアップ

* `pip install langchain`でPythonパッケージをインストールしてください
  > Install the Python package with `pip install langchain`

## Wrappers | ラッパー

### LLM | LLM

NIBittensor LLMラッパーが存在し、以下の方法でアクセスできます：

> There exists a NIBittensor LLM wrapper, which you can access with:

```python
from langchain.llms import NIBittensorLLM
```

すべてのモデルに対して統一されたインターフェースを提供します：

> It provides a unified interface for all models:

```python
llm = NIBittensorLLM(system_prompt="Your task is to provide concise and accurate response based on user prompt")

print(llm('Write a fibonacci function in python with golder ratio'))
```

トップマイナーからの複数のレスポンスは、`top_responses`パラメータを使用してアクセスできます：

> Multiple responses from top miners can be accessible using the `top_responses` parameter:

```python
multi_response_llm = NIBittensorLLM(top_responses=10)
multi_resp = multi_response_llm("What is Neural Network Feeding Mechanism?")
json_multi_resp = json.loads(multi_resp)

print(json_multi_resp)
```
