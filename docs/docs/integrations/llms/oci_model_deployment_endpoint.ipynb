{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCI Data Science Model Deployment Endpoint | OCIデータサイエンスモデルデプロイメントエンドポイント\n",
    "\n",
    "[OCI Data Science](https://docs.oracle.com/en-us/iaas/data-science/using/home.htm)は、データサイエンスチームがOracle Cloud Infrastructure内で機械学習モデルを構築、訓練、そして管理するための完全に管理されたサーバーレスプラットフォームです。\n",
    "\n",
    "> [OCI Data Science](https://docs.oracle.com/en-us/iaas/data-science/using/home.htm) is a fully managed and serverless platform for data science teams to build, train, and manage machine learning models in the Oracle Cloud Infrastructure.\n",
    "\n",
    "このノートブックでは、[OCI Data Science Model Deployment](https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-about.htm)にホストされているLLMの使用方法について説明します。\n",
    "\n",
    "> This notebooks goes over how to use an LLM hosted on a [OCI Data Science Model Deployment](https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-about.htm).\n",
    "\n",
    "認証には、[oracle-ads](https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html)が使用され、エンドポイントを呼び出すための資格情報を自動的に読み込むように設定されています。\n",
    "\n",
    "> To authenticate, [oracle-ads](https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html) has been used to automatically load credentials for invoking endpoint.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install oracle-ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite | 前提条件\n",
    "\n",
    "### Deploy model | モデルをデプロイする\n",
    "\n",
    "OCI Data Scienceのモデルデプロイメントであなたのllmをデプロイする方法については、[Oracle GitHubサンプルリポジトリ](https://github.com/oracle-samples/oci-data-science-ai-samples/tree/main/model-deployment/containers/llama2)を確認してください。\n",
    "\n",
    "> Check [Oracle GitHub samples repository](https://github.com/oracle-samples/oci-data-science-ai-samples/tree/main/model-deployment/containers/llama2) on how to deploy your llm on OCI Data Science Model deployment.\n",
    "\n",
    "### Policies | ポリシー\n",
    "\n",
    "OCI Data Scienceモデルデプロイメントエンドポイントにアクセスするために必要な[policies](https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-policies-auth.htm#model_dep_policies_auth__predict-endpoint)を確認してください。\n",
    "\n",
    "> Make sure to have the required [policies](https://docs.oracle.com/en-us/iaas/data-science/using/model-dep-policies-auth.htm#model_dep_policies_auth__predict-endpoint) to access the OCI Data Science Model Deployment endpoint.\n",
    "\n",
    "## Set up | セットアップ\n",
    "\n",
    "### vLLM | vLLM\n",
    "\n",
    "モデルをデプロイした後、`OCIModelDeploymentVLLM` コールの以下の必須パラメータを設定する必要があります：\n",
    "\n",
    "> After having deployed model, you have to set up following required parameters of the `OCIModelDeploymentVLLM` call:\n",
    "\n",
    "* **`endpoint`**: デプロイされたモデルのモデルHTTPエンドポイントです。例えば `https://<MD_OCID>/predict` のようになります。\n",
    "\n",
    "  > **`endpoint`**: The model HTTP endpoint from the deployed model, e.g. `https://<MD_OCID>/predict`.\n",
    "\n",
    "* **`model`**: モデルの場所。\n",
    "\n",
    "  > **`model`**: The location of the model.\n",
    "\n",
    "\n",
    "### Text generation inference (TGI) | テキスト生成推論（TGI）\n",
    "\n",
    "`OCIModelDeploymentTGI` コールの以下の必須パラメータを設定する必要があります：\n",
    "\n",
    "> You have to set up following required parameters of the `OCIModelDeploymentTGI` call:\n",
    "\n",
    "* **`endpoint`**: デプロイされたモデルのHTTPエンドポイントです。例えば `https://<MD_OCID>/predict` のようになります。\n",
    "\n",
    "  > **`endpoint`**: The model HTTP endpoint from the deployed model, e.g. `https://<MD_OCID>/predict`.\n",
    "\n",
    "\n",
    "### Authentication | 認証\n",
    "\n",
    "認証は、APIキーまたは環境変数を通じて設定することができます。OCIデータサイエンスノートブックセッションで作業している場合、リソースプリンシパルを利用して他のOCIリソースにアクセスすることができます。さらに多くのオプションについては[こちら](https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html)をチェックしてください。\n",
    "\n",
    "> You can set authentication through either ads or environment variables. When you are working in OCI Data Science Notebook Session, you can leverage resource principal to access other OCI resources. Check out [here](https://accelerated-data-science.readthedocs.io/en/latest/user_guide/cli/authentication.html) to see more options.\n",
    "\n",
    "## Example | 例\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ads\n",
    "from langchain_community.llms import OCIModelDeploymentVLLM\n",
    "\n",
    "# Set authentication through ads\n",
    "# Use resource principal are operating within a\n",
    "# OCI service that has resource principal based\n",
    "# authentication configured\n",
    "ads.set_auth(\"resource_principal\")\n",
    "\n",
    "# Create an instance of OCI Model Deployment Endpoint\n",
    "# Replace the endpoint uri and model name with your own\n",
    "llm = OCIModelDeploymentVLLM(endpoint=\"https://<MD_OCID>/predict\", model=\"model_name\")\n",
    "\n",
    "# Run the LLM\n",
    "llm.invoke(\"Who is the first president of United States?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_community.llms import OCIModelDeploymentTGI\n",
    "\n",
    "# Set authentication through environment variables\n",
    "# Use API Key setup when you are working from a local\n",
    "# workstation or on platform which does not support\n",
    "# resource principals.\n",
    "os.environ[\"OCI_IAM_TYPE\"] = \"api_key\"\n",
    "os.environ[\"OCI_CONFIG_PROFILE\"] = \"default\"\n",
    "os.environ[\"OCI_CONFIG_LOCATION\"] = \"~/.oci\"\n",
    "\n",
    "# Set endpoint through environment variables\n",
    "# Replace the endpoint uri with your own\n",
    "os.environ[\"OCI_LLM_ENDPOINT\"] = \"https://<MD_OCID>/predict\"\n",
    "\n",
    "# Create an instance of OCI Model Deployment Endpoint\n",
    "llm = OCIModelDeploymentTGI()\n",
    "\n",
    "# Run the LLM\n",
    "llm.invoke(\"Who is the first president of United States?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}