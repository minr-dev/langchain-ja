{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 2\n",
    "title: Multi-language anonymization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-language data anonymization with Microsoft Presidio | Microsoft Presidioによる多言語データの匿名化\n",
    "\n",
    "[![Colabで開く](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/guides/privacy/presidio_data_anonymization/multi_language.ipynb)\n",
    "\n",
    "> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain/blob/master/docs/docs/guides/privacy/presidio_data_anonymization/multi_language.ipynb)\n",
    "\n",
    "## Use case | 使用例\n",
    "\n",
    "データの擬似名化における多言語サポートは、言語構造と文化的文脈の違いにより不可欠です。異なる言語では、個人識別子の形式が異なる場合があります。例えば、名前、場所、日付の構造は言語や地域によって大きく異なることがあります。さらに、非アルファベット文字、アクセント、書き方の方向が擬似名化プロセスに影響を与える可能性があります。多言語サポートがなければ、データが識別可能なまま残るか、誤解されることで、データのプライバシーと正確性が損なわれます。したがって、グローバルなオペレーションに適した効果的で正確な擬似名化を実現するためには、多言語サポートが必要です。\n",
    "\n",
    "> Multi-language support in data pseudonymization is essential due to differences in language structures and cultural contexts. Different languages may have varying formats for personal identifiers. For example, the structure of names, locations and dates can differ greatly between languages and regions. Furthermore, non-alphanumeric characters, accents, and the direction of writing can impact pseudonymization processes. Without multi-language support, data could remain identifiable or be misinterpreted, compromising data privacy and accuracy. Hence, it enables effective and precise pseudonymization suited for global operations.\n",
    "\n",
    "## Overview | 概要\n",
    "\n",
    "Microsoft Presidioにおける個人識別情報（PII）の検出は、いくつかのコンポーネントに依存しています。通常のパターンマッチング（例えば、正規表現を使用する）に加えて、アナライザーはNamed Entity Recognition（NER）のモデルを使用して、以下のようなエンティティを抽出します：\n",
    "\n",
    "> PII detection in Microsoft Presidio relies on several components - in addition to the usual pattern matching (e.g. using regex), the analyser uses a model for Named Entity Recognition (NER) to extract entities such as:\n",
    "\n",
    "* `PERSON`\n",
    "\n",
    "  > `PERSON`\n",
    "\n",
    "* `LOCATION`\n",
    "\n",
    "  > `LOCATION`\n",
    "\n",
    "* `DATE_TIME`\n",
    "\n",
    "  > `DATE_TIME`\n",
    "\n",
    "* `NRP`\n",
    "\n",
    "  > `NRP`\n",
    "\n",
    "* `ORGANIZATION`\n",
    "\n",
    "  > `ORGANIZATION`\n",
    "\n",
    "\n",
    "[\\[ソース\\]](https://github.com/microsoft/presidio/blob/main/presidio-analyzer/presidio_analyzer/predefined_recognizers/spacy_recognizer.py)\n",
    "\n",
    "> [\\[Source\\]](https://github.com/microsoft/presidio/blob/main/presidio-analyzer/presidio_analyzer/predefined_recognizers/spacy_recognizer.py)\n",
    "\n",
    "特定の言語での固有表現認識（NER）を処理するために、私たちは`spaCy`ライブラリの独自のモデルを使用しています。このライブラリは、多様な言語とサイズをカバーする広範な選択肢で認識されています。しかし、必要に応じて[Stanza](https://microsoft.github.io/presidio/analyzer/nlp_engines/spacy_stanza/)や[transformers](https://microsoft.github.io/presidio/analyzer/nlp_engines/transformers/)などの代替フレームワークとの統合も可能で、制限はありません。\n",
    "\n",
    "> To handle NER in specific languages, we utilize unique models from the `spaCy` library, recognized for its extensive selection covering multiple languages and sizes. However, it's not restrictive, allowing for integration of alternative frameworks such as [Stanza](https://microsoft.github.io/presidio/analyzer/nlp_engines/spacy_stanza/) or [transformers](https://microsoft.github.io/presidio/analyzer/nlp_engines/transformers/) when necessary.\n",
    "\n",
    "## Quickstart | クイックスタート\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# ! pip install langchain langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker\n",
    "# ! python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.data_anonymizer import PresidioReversibleAnonymizer\n",
    "\n",
    "anonymizer = PresidioReversibleAnonymizer(\n",
    "    analyzed_fields=[\"PERSON\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デフォルトでは、`PresidioAnonymizer`と`PresidioReversibleAnonymizer`は英語のテキストで訓練されたモデルを使用しているため、他の言語に対しても適度にうまく対応します。\n",
    "\n",
    "> By default, `PresidioAnonymizer` and `PresidioReversibleAnonymizer` use a model trained on English texts, so they handle other languages moderately well.\n",
    "\n",
    "例えば、ここではモデルが人物を検出できませんでした：\n",
    "\n",
    "> For example, here the model did not detect the person:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Me llamo Sofía'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anonymizer.anonymize(\"Me llamo Sofía\")  # \"My name is Sofía\" in Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、他の言語の単語を実際のエンティティとして取り扱うこともあります。ここでは、スペイン語で「私」を意味する\\*'Yo'*と*Sofía\\*が、`PERSON`として分類されています：\n",
    "\n",
    "> They may also take words from another language as actual entities. Here, both the word *'Yo'* (*'I'* in Spanish) and *Sofía* have been classified as `PERSON`:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kari Lopez soy Mary Walker'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anonymizer.anonymize(\"Yo soy Sofía\")  # \"I am Sofía\" in Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他の言語のテキストを匿名化したい場合は、他のモデルをダウンロードして、匿名化設定に追加する必要があります：\n",
    "\n",
    "> If you want to anonymise texts from other languages, you need to download other models and add them to the anonymiser configuration:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the models for the languages you want to use\n",
    "# ! python -m spacy download en_core_web_md\n",
    "# ! python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_config = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [\n",
    "        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_md\"},\n",
    "        {\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そのため、スペイン語の言語モデルを追加しました。また、英語の代替モデルもダウンロードしました - この場合、大きなモデル `en_core_web_lg`（560MB）をより小さいバージョンの `en_core_web_md`（40MB）に置き換えました - サイズは従って14分の1に削減されています！匿名化の速度が重要な場合は、これを検討する価値があるでしょう。\n",
    "\n",
    "> We have therefore added a Spanish language model. Note also that we have downloaded an alternative model for English as well - in this case we have replaced the large model `en_core_web_lg` (560MB) with its smaller version `en_core_web_md` (40MB) - the size is therefore reduced by 14 times! If you care about the speed of anonymisation, it is worth considering it.\n",
    "\n",
    "異なる言語のすべてのモデルは、[spaCyのドキュメント](https://spacy.io/usage/models)で見つけることができます。\n",
    "\n",
    "> All models for the different languages can be found in the [spaCy documentation](https://spacy.io/usage/models).\n",
    "\n",
    "次に、設定を`languages_config`パラメータとしてAnonymiserに渡してください。ご覧の通り、前述の例はどちらも問題なく動作します：\n",
    "\n",
    "> Now pass the configuration as the `languages_config` parameter to Anonymiser. As you can see, both previous examples work flawlessly:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me llamo Christopher Smith\n",
      "Yo soy Joseph Jenkins\n"
     ]
    }
   ],
   "source": [
    "anonymizer = PresidioReversibleAnonymizer(\n",
    "    analyzed_fields=[\"PERSON\"],\n",
    "    languages_config=nlp_config,\n",
    ")\n",
    "\n",
    "print(\n",
    "    anonymizer.anonymize(\"Me llamo Sofía\", language=\"es\")\n",
    ")  # \"My name is Sofía\" in Spanish\n",
    "print(anonymizer.anonymize(\"Yo soy Sofía\", language=\"es\"))  # \"I am Sofía\" in Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デフォルトでは、テキストを匿名化する際には設定で最初に指定された言語が使用されます（この場合、英語です）：\n",
    "\n",
    "> By default, the language indicated first in the configuration will be used when anonymising text (in this case English):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Shawna Bennett\n"
     ]
    }
   ],
   "source": [
    "print(anonymizer.anonymize(\"My name is John\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage with other frameworks | 他のフレームワークとの使用方法\n",
    "\n",
    "### Language detection | 言語検出\n",
    "\n",
    "提示されたアプローチの欠点の一つは、入力テキストの**言語**を直接指定しなければならないことです。しかし、その対策として*言語検出*ライブラリがあります。\n",
    "\n",
    "> One of the drawbacks of the presented approach is that we have to pass the **language** of the input text directly. However, there is a remedy for that - *language detection* libraries.\n",
    "\n",
    "以下のフレームワークのいずれかを使用することをお勧めします：\n",
    "\n",
    "> We recommend using one of the following frameworks:\n",
    "\n",
    "* fasttext（推奨）\n",
    "\n",
    "  > fasttext (recommended)\n",
    "\n",
    "* langdetect\n",
    "\n",
    "  > langdetect\n",
    "\n",
    "\n",
    "私たちの経験によると、*fasttext* は少し優れていると思われますが、ご自身のユースケースで検証することをお勧めします。\n",
    "\n",
    "> From our experience *fasttext* performs a bit better, but you should verify it on your use case.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# ! pip install fasttext langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langdetect | langdetect\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "from langchain.schema import runnable\n",
    "\n",
    "\n",
    "def detect_language(text: str) -> dict:\n",
    "    language = langdetect.detect(text)\n",
    "    print(language)\n",
    "    return {\"text\": text, \"language\": language}\n",
    "\n",
    "\n",
    "chain = runnable.RunnableLambda(detect_language) | (\n",
    "    lambda x: anonymizer.anonymize(x[\"text\"], language=x[\"language\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Me llamo Michael Perez III'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Me llamo Sofía\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My name is Ronald Bennett'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"My name is John Doe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fasttext | fasttext\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz からfasttextモデルをダウンロードする必要があります。\n",
    "\n",
    "> You need to download the fasttext model first from https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "model = fasttext.load_model(\"lid.176.ftz\")\n",
    "\n",
    "\n",
    "def detect_language(text: str) -> dict:\n",
    "    language = model.predict(text)[0][0].replace(\"__label__\", \"\")\n",
    "    print(language)\n",
    "    return {\"text\": text, \"language\": language}\n",
    "\n",
    "\n",
    "chain = runnable.RunnableLambda(detect_language) | (\n",
    "    lambda x: anonymizer.anonymize(x[\"text\"], language=x[\"language\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yo soy Angela Werner'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Yo soy Sofía\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My name is Carlos Newton'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"My name is John Doe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この方法では、関連する言語に対応するエンジンでモデルを初期化するだけで済み、ツールの使用は完全に自動化されています。\n",
    "\n",
    "> This way you only need to initialize the model with the engines corresponding to the relevant languages, but using the tool is fully automated.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced usage | 高度な使い方\n",
    "\n",
    "### Custom labels in NER model | NERモデルのカスタムラベル\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "たとえば、spaCyモデルには、デフォルトでMicrosoft Presidioがサポートするクラス名とは異なるものがあるかもしれません。ポーランド語を例にとると：\n",
    "\n",
    "> It may be that the spaCy model has different class names than those supported by the Microsoft Presidio by default. Take Polish, for example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Wiktoria, Start: 12, End: 20, Label: persName\n"
     ]
    }
   ],
   "source": [
    "# ! python -m spacy download pl_core_news_md\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"pl_core_news_md\")\n",
    "doc = nlp(\"Nazywam się Wiktoria\")  # \"My name is Wiktoria\" in Polish\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(\n",
    "        f\"Text: {ent.text}, Start: {ent.start_char}, End: {ent.end_char}, Label: {ent.label_}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "名前 *Victoria* は `persName` として分類されましたが、これは Microsoft Presidio において実装されているデフォルトのクラス名 `PERSON`/`PER` とは対応していません（[SpacyRecognizerの実装](https://github.com/microsoft/presidio/blob/main/presidio-analyzer/presidio_analyzer/predefined_recognizers/spacy_recognizer.py)で `CHECK_LABEL_GROUPS` を探してください）。\n",
    "\n",
    "> The name *Victoria* was classified as `persName`, which does not correspond to the default class names `PERSON`/`PER` implemented in Microsoft Presidio (look for `CHECK_LABEL_GROUPS` in [SpacyRecognizer implementation](https://github.com/microsoft/presidio/blob/main/presidio-analyzer/presidio_analyzer/predefined_recognizers/spacy_recognizer.py)).\n",
    "\n",
    "spaCyモデルのカスタムラベル（自分でトレーニングしたものを含む）についての詳細は、[このスレッド](https://github.com/microsoft/presidio/issues/851)で確認できます。\n",
    "\n",
    "> You can find out more about custom labels in spaCy models (including your own, trained ones) in [this thread](https://github.com/microsoft/presidio/issues/851).\n",
    "\n",
    "そのため、私たちの文は匿名化されません：\n",
    "\n",
    "> That's why our sentence will not be anonymized:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nazywam się Wiktoria\n"
     ]
    }
   ],
   "source": [
    "nlp_config = {\n",
    "    \"nlp_engine_name\": \"spacy\",\n",
    "    \"models\": [\n",
    "        {\"lang_code\": \"en\", \"model_name\": \"en_core_web_md\"},\n",
    "        {\"lang_code\": \"es\", \"model_name\": \"es_core_news_md\"},\n",
    "        {\"lang_code\": \"pl\", \"model_name\": \"pl_core_news_md\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "anonymizer = PresidioReversibleAnonymizer(\n",
    "    analyzed_fields=[\"PERSON\", \"LOCATION\", \"DATE_TIME\"],\n",
    "    languages_config=nlp_config,\n",
    ")\n",
    "\n",
    "print(\n",
    "    anonymizer.anonymize(\"Nazywam się Wiktoria\", language=\"pl\")\n",
    ")  # \"My name is Wiktoria\" in Polish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これに対処するために、独自のクラスマッピングを持つ`SpacyRecognizer`を作成し、それを匿名化ツールに追加してください：\n",
    "\n",
    "> To address this, create your own `SpacyRecognizer` with your own class mapping and add it to the anonymizer:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer.predefined_recognizers import SpacyRecognizer\n",
    "\n",
    "polish_check_label_groups = [\n",
    "    ({\"LOCATION\"}, {\"placeName\", \"geogName\"}),\n",
    "    ({\"PERSON\"}, {\"persName\"}),\n",
    "    ({\"DATE_TIME\"}, {\"date\", \"time\"}),\n",
    "]\n",
    "\n",
    "spacy_recognizer = SpacyRecognizer(\n",
    "    supported_language=\"pl\",\n",
    "    check_label_groups=polish_check_label_groups,\n",
    ")\n",
    "\n",
    "anonymizer.add_recognizer(spacy_recognizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで全てがスムーズに機能します：\n",
    "\n",
    "> Now everything works smoothly:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nazywam się Morgan Walters\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    anonymizer.anonymize(\"Nazywam się Wiktoria\", language=\"pl\")\n",
    ")  # \"My name is Wiktoria\" in Polish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "もっと複雑な例を試してみましょう：\n",
    "\n",
    "> Let's try on more complex example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nazywam się Ernest Liu. New Taylorburgh to moje miasto rodzinne. Urodziłam się 1987-01-19\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    anonymizer.anonymize(\n",
    "        \"Nazywam się Wiktoria. Płock to moje miasto rodzinne. Urodziłam się dnia 6 kwietnia 2001 roku\",\n",
    "        language=\"pl\",\n",
    "    )\n",
    ")  # \"My name is Wiktoria. Płock is my home town. I was born on 6 April 2001\" in Polish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ご覧の通り、クラスマッピングのおかげで、匿名化ツールはさまざまなタイプのエンティティに対応できます。\n",
    "\n",
    "> As you can see, thanks to class mapping, the anonymiser can cope with different types of entities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom language-specific operators | 言語固有のカスタムオペレーター\n",
    "\n",
    "上記の例では、文章は正しく匿名化されていますが、偽のデータはポーランド語に全く適していません。したがって、この問題を解決するためにカスタムオペレーターを追加できます：\n",
    "\n",
    "> In the example above, the sentence has been anonymised correctly, but the fake data does not fit the Polish language at all. Custom operators can therefore be added, which will resolve the issue:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "\n",
    "fake = Faker(locale=\"pl_PL\")  # Setting faker to provide Polish data\n",
    "\n",
    "new_operators = {\n",
    "    \"PERSON\": OperatorConfig(\"custom\", {\"lambda\": lambda _: fake.first_name_female()}),\n",
    "    \"LOCATION\": OperatorConfig(\"custom\", {\"lambda\": lambda _: fake.city()}),\n",
    "}\n",
    "\n",
    "anonymizer.add_operators(new_operators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nazywam się Marianna. Szczecin to moje miasto rodzinne. Urodziłam się 1976-11-16\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    anonymizer.anonymize(\n",
    "        \"Nazywam się Wiktoria. Płock to moje miasto rodzinne. Urodziłam się dnia 6 kwietnia 2001 roku\",\n",
    "        language=\"pl\",\n",
    "    )\n",
    ")  # \"My name is Wiktoria. Płock is my home town. I was born on 6 April 2001\" in Polish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations | 制限事項\n",
    "\n",
    "覚えておいてください - 結果は、あなたの認識器とNERモデルの質によって決まります！\n",
    "\n",
    "> Remember - results are as good as your recognizers and as your NER models!\n",
    "\n",
    "以下の例を見てください。スペイン語の小さいモデル（12MB）をダウンロードしましたが、中サイズのバージョン（40MB）ほどの性能は発揮できませんでした：\n",
    "\n",
    "> Look at the example below - we downloaded the small model for Spanish (12MB) and it no longer performs as well as the medium version (40MB):\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: es_core_news_sm. Result: Me llamo Sofía\n",
      "Model: es_core_news_md. Result: Me llamo Lawrence Davis\n"
     ]
    }
   ],
   "source": [
    "# ! python -m spacy download es_core_news_sm\n",
    "\n",
    "for model in [\"es_core_news_sm\", \"es_core_news_md\"]:\n",
    "    nlp_config = {\n",
    "        \"nlp_engine_name\": \"spacy\",\n",
    "        \"models\": [\n",
    "            {\"lang_code\": \"es\", \"model_name\": model},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    anonymizer = PresidioReversibleAnonymizer(\n",
    "        analyzed_fields=[\"PERSON\"],\n",
    "        languages_config=nlp_config,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Model: {model}. Result: {anonymizer.anonymize('Me llamo Sofía', language='es')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多くの場合、spaCyの大規模なモデルでさえ十分ではないことがあります - 名前付きエンティティを検出するための、トランスフォーマーに基づく他の、より複雑で優れた方法が既に存在しています。これについての詳細は[こちら](https://microsoft.github.io/presidio/analyzer/nlp_engines/transformers/)で読むことができます。\n",
    "\n",
    "> In many cases, even the larger models from spaCy will not be sufficient - there are already other, more complex and better methods of detecting named entities, based on transformers. You can read more about this [here](https://microsoft.github.io/presidio/analyzer/nlp_engines/transformers/).\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}