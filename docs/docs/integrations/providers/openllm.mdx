# OpenLLM | OpenLLM

このページでは、[OpenLLM](https://github.com/bentoml/OpenLLM)をLangChainで使用する方法を示しています。

> This page demonstrates how to use [OpenLLM](https://github.com/bentoml/OpenLLM)
> with LangChain.

`OpenLLM`は、大規模言語モデル（LLMs）を本番環境で運用するためのオープンプラットフォームです。開発者は、任意のオープンソースLLMsで簡単に推論を実行し、クラウドまたはオンプレミス環境にデプロイし、強力なAIアプリを構築することができます。

> `OpenLLM` is an open platform for operating large language models (LLMs) in
> production. It enables developers to easily run inference with any open-source
> LLMs, deploy to the cloud or on-premises, and build powerful AI apps.

## Installation and Setup | インストールとセットアップ

PyPIを通じてOpenLLMパッケージをインストールしてください：

> Install the OpenLLM package via PyPI:

```bash
pip install openllm
```

## LLM | LLM

OpenLLMは、オープンソースのLLMを幅広くサポートするだけでなく、ユーザー自身がファインチューニングしたLLMを利用することもできます。`openllm model` コマンドを使用して、OpenLLMに事前に最適化された利用可能な全てのモデルを確認してください。

> OpenLLM supports a wide range of open-source LLMs as well as serving users' own
> fine-tuned LLMs. Use `openllm model` command to see all available models that
> are pre-optimized for OpenLLM.

## Wrappers | ラッパー

OpenLLM Wrapperがあり、プロセス内でLLMをロードするか、リモートのOpenLLMサーバーにアクセスすることをサポートしています。

> There is a OpenLLM Wrapper which supports loading LLM in-process or accessing a
> remote OpenLLM server:

```python
from langchain.llms import OpenLLM
```

### Wrapper for OpenLLM server | OpenLLMサーバーのラッパー

このラッパーは、HTTPまたはgRPCを介してOpenLLMサーバーに接続することをサポートしています。OpenLLMサーバーは、ローカルまたはクラウド上で実行できます。

> This wrapper supports connecting to an OpenLLM server via HTTP or gRPC. The
> OpenLLM server can run either locally or on the cloud.

ローカルで試すには、OpenLLMサーバーを起動してください：

> To try it out locally, start an OpenLLM server:

```bash
openllm start flan-t5
```

ラッパーの使用法：

> Wrapper usage:

```python
from langchain.llms import OpenLLM

llm = OpenLLM(server_url='http://localhost:3000')

llm("What is the difference between a duck and a goose? And why there are so many Goose in Canada?")
```

### Wrapper for Local Inference | ローカル推論用のラッパー

OpenLLMラッパーを使用して、現在のPythonプロセス内でLLMをロードし、推論を実行することもできます。

> You can also use the OpenLLM wrapper to load LLM in current Python process for
> running inference.

```python
from langchain.llms import OpenLLM

llm = OpenLLM(model_name="dolly-v2", model_id='databricks/dolly-v2-7b')

llm("What is the difference between a duck and a goose? And why there are so many Goose in Canada?")
```

### Usage | 使用方法

OpenLLM Wrapperのより詳細なウォークスルーについては、[サンプルノートブック](/docs/integrations/llms/openllm)をご覧ください。

> For a more detailed walkthrough of the OpenLLM Wrapper, see the
> [example notebook](/docs/integrations/llms/openllm)
