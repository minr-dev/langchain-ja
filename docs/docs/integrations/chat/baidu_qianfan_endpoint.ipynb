{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_label: Baidu Qianfan\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QianfanChatEndpoint | QianfanChatEndpoint",
    "",
    "Baidu AI Cloud Qianfan Platformは、企業開発者向けのワンストップ大規模モデル開発およびサービス運用プラットフォームです。Qianfanは、Wenxin Yiyan（ERNIE-Bot）のモデルやサードパーティのオープンソースモデルを含むだけでなく、様々なAI開発ツールや完全な開発環境のセットも提供し、顧客が大規模モデルアプリケーションを容易に使用し、開発できるよう支援します。",
    "",
    "> Baidu AI Cloud Qianfan Platform is a one-stop large model development and service operation platform for enterprise developers. Qianfan not only provides including the model of Wenxin Yiyan (ERNIE-Bot) and the third-party open-source models, but also provides various AI development tools and the whole set of development environment, which facilitates customers to use and develop large model applications easily.",
    "",
    "基本的に、それらのモデルは以下のタイプに分けられます：",
    "",
    "> Basically, those model are split into the following type:",
    "",
    "* 埋め込み",
    "  > Embedding",
    "* チャット",
    "  > Chat",
    "* 完了",
    "  > Completion",
    "",
    "このノートブックでは、`langchain/chat_models`パッケージに相当する`Chat`を主に使用して、[Qianfan](https://cloud.baidu.com/doc/WENXINWORKSHOP/index.html)と共にlangchainの使用方法を紹介します。",
    "",
    "> In this notebook, we will introduce how to use langchain with [Qianfan](https://cloud.baidu.com/doc/WENXINWORKSHOP/index.html) mainly in `Chat` corresponding",
    "> to the package `langchain/chat_models` in langchain:",
    "",
    "## API Initialization | APIの初期化",
    "",
    "Baidu Qianfanに基づくLLMサービスを使用するには、これらのパラメータを初期化する必要があります：",
    "",
    "> To use the LLM services based on Baidu Qianfan, you have to initialize these parameters:",
    "",
    "AKとSKを環境変数で初期化するか、初期化パラメータで初期化するか選択できます：",
    "",
    "> You could either choose to init the AK,SK in environment variables or init params:",
    "",
    "```base",
    "export QIANFAN_AK=XXX",
    "export QIANFAN_SK=XXX",
    "```",
    "",
    "## Current supported models: | 現在サポートされているモデル：",
    "",
    "* ERNIE-Bot-turbo（デフォルトモデル）",
    "  > ERNIE-Bot-turbo (default models)",
    "* ERNIE-Bot",
    "  > ERNIE-Bot",
    "* BLOOMZ-7B",
    "  > BLOOMZ-7B",
    "* Llama-2-7b-chat",
    "  > Llama-2-7b-chat",
    "* Llama-2-13b-chat",
    "  > Llama-2-13b-chat",
    "* Llama-2-70b-chat",
    "  > Llama-2-70b-chat",
    "* Qianfan-BLOOMZ-7B-compressed",
    "  > Qianfan-BLOOMZ-7B-compressed",
    "* Qianfan-Chinese-Llama-2-7B",
    "  > Qianfan-Chinese-Llama-2-7B",
    "* ChatGLM2-6B-32K",
    "  > ChatGLM2-6B-32K",
    "* AquilaChat-7B",
    "  > AquilaChat-7B",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [09-15 20:00:29] logging.py:55 [t:139698882193216]: requesting llm api endpoint: /chat/eb-instant\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For basic init and call\"\"\"\n",
    "import os\n",
    "\n",
    "from langchain.chat_models import QianfanChatEndpoint\n",
    "from langchain_core.language_models.chat_models import HumanMessage\n",
    "\n",
    "os.environ[\"QIANFAN_AK\"] = \"your_ak\"\n",
    "os.environ[\"QIANFAN_SK\"] = \"your_sk\"\n",
    "\n",
    "chat = QianfanChatEndpoint(\n",
    "    streaming=True,\n",
    ")\n",
    "res = chat([HumanMessage(content=\"write a funny joke\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [09-15 20:00:36] logging.py:55 [t:139698882193216]: requesting llm api endpoint: /chat/eb-instant\n",
      "[INFO] [09-15 20:00:37] logging.py:55 [t:139698882193216]: async requesting llm api endpoint: /chat/eb-instant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat resp: content='您好，您似乎输入' additional_kwargs={} example=False\n",
      "chat resp: content='了一个话题标签，请问需要我帮您找到什么资料或者帮助您解答什么问题吗？' additional_kwargs={} example=False\n",
      "chat resp: content='' additional_kwargs={} example=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [09-15 20:00:39] logging.py:55 [t:139698882193216]: async requesting llm api endpoint: /chat/eb-instant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[ChatGeneration(text=\"The sea is a vast expanse of water that covers much of the Earth's surface. It is a source of travel, trade, and entertainment, and is also a place of scientific exploration and marine conservation. The sea is an important part of our world, and we should cherish and protect it.\", generation_info={'finish_reason': 'finished'}, message=AIMessage(content=\"The sea is a vast expanse of water that covers much of the Earth's surface. It is a source of travel, trade, and entertainment, and is also a place of scientific exploration and marine conservation. The sea is an important part of our world, and we should cherish and protect it.\", additional_kwargs={}, example=False))]] llm_output={} run=[RunInfo(run_id=UUID('d48160a6-5960-4c1d-8a0e-90e6b51a209b'))]\n",
      "astream content='The sea is a vast' additional_kwargs={} example=False\n",
      "astream content=' expanse of water, a place of mystery and adventure. It is the source of many cultures and civilizations, and a center of trade and exploration. The sea is also a source of life and beauty, with its unique marine life and diverse' additional_kwargs={} example=False\n",
      "astream content=' coral reefs. Whether you are swimming, diving, or just watching the sea, it is a place that captivates the imagination and transforms the spirit.' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import QianfanChatEndpoint\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chatLLM = QianfanChatEndpoint(\n",
    "    streaming=True,\n",
    ")\n",
    "res = chatLLM.stream([HumanMessage(content=\"hi\")], streaming=True)\n",
    "for r in res:\n",
    "    print(\"chat resp:\", r)\n",
    "\n",
    "\n",
    "async def run_aio_generate():\n",
    "    resp = await chatLLM.agenerate(\n",
    "        messages=[[HumanMessage(content=\"write a 20 words sentence about sea.\")]]\n",
    "    )\n",
    "    print(resp)\n",
    "\n",
    "\n",
    "await run_aio_generate()\n",
    "\n",
    "\n",
    "async def run_aio_stream():\n",
    "    async for res in chatLLM.astream(\n",
    "        [HumanMessage(content=\"write a 20 words sentence about sea.\")]\n",
    "    ):\n",
    "        print(\"astream\", res)\n",
    "\n",
    "\n",
    "await run_aio_stream()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use different models in Qianfan | Qianfanで異なるモデルを使用する",
    "",
    "Ernie Botやサードパーティのオープンソースモデルに基づいて自分のモデルをデプロイしたい場合は、以下の手順に従ってください：",
    "",
    "> In the case you want to deploy your own model based on Ernie Bot or third-party open-source model, you could follow these steps:",
    "",
    "* 1. （オプショナル、もしモデルがデフォルトモデルに含まれている場合はこれをスキップしてください）Qianfanコンソールであなたのモデルをデプロイし、あなた専用のカスタマイズされたデプロイエンドポイントを取得してください。",
    "     > （Optional, if the model are included in the default models, skip it）Deploy your model in Qianfan Console, get your own customized deploy endpoint.",
    "* 2. 初期化の際に`endpoint`というフィールドを設定してください。",
    "     > Set up the field called `endpoint` in the initialization:",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [09-15 20:00:50] logging.py:55 [t:139698882193216]: requesting llm api endpoint: /chat/bloomz_7b1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='你好！很高兴见到你。' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "chatBloom = QianfanChatEndpoint(\n",
    "    streaming=True,\n",
    "    model=\"BLOOMZ-7B\",\n",
    ")\n",
    "res = chatBloom([HumanMessage(content=\"hi\")])\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Params: | モデルパラメータ:",
    "",
    "現時点では、`ERNIE-Bot` と `ERNIE-Bot-turbo` のみが以下に示すモデルパラメータをサポートしていますが、将来的にはさらに多くのモデルをサポートする可能性があります。",
    "",
    "> For now, only `ERNIE-Bot` and `ERNIE-Bot-turbo` support model params below, we might support more models in the future.",
    "",
    "* 温度",
    "  > temperature",
    "* top\\_p",
    "  > top\\_p",
    "* ペナルティスコア",
    "  > penalty\\_score",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [09-15 20:00:57] logging.py:55 [t:139698882193216]: requesting llm api endpoint: /chat/eb-instant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='您好，您似乎输入' additional_kwargs={} example=False\n",
      "content='了一个文本字符串，但并没有给出具体的问题或场景。' additional_kwargs={} example=False\n",
      "content='如果您能提供更多信息，我可以更好地回答您的问题。' additional_kwargs={} example=False\n",
      "content='' additional_kwargs={} example=False\n"
     ]
    }
   ],
   "source": [
    "res = chat.stream(\n",
    "    [HumanMessage(content=\"hi\")],\n",
    "    **{\"top_p\": 0.4, \"temperature\": 0.1, \"penalty_score\": 1},\n",
    ")\n",
    "\n",
    "for r in res:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6fa70026b407ae751a5c9e6bd7f7d482379da8ad616f98512780b705c84ee157"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}