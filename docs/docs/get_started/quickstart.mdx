# Quickstart | クイックスタート

このクイックスタートガイドでは、以下の方法をご紹介します：

> In this quickstart we'll show you how to:

* LangChain、LangSmith、LangServeのセットアップ方法を学びます
  > Get setup with LangChain, LangSmith and LangServe
* LangChainの最も基本的かつ一般的なコンポーネントを使用しましょう：プロンプトテンプレート、モデル、そして出力パーサー
  > Use the most basic and common components of LangChain: prompt templates, models, and output parsers
* LangChain Expression Languageを使用してください。これはLangChainが構築されているプロトコルであり、コンポーネント間の連携を容易にするものです。
  > Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining
* LangChainを用いたシンプルなアプリケーションの作成
  > Build a simple application with LangChain
* LangSmithを用いてアプリケーションのトレースを行う
  > Trace your application with LangSmith
* LangServeを用いてアプリケーションを稼働させる
  > Serve your application with LangServe

それは相当な量ですね！さっそく始めましょう。

> That's a fair amount to cover! Let's dive in.

## Setup | セットアップ

### Installation | インストール方法

LangChainをインストールするには、次のコマンドを実行してください：

> To install LangChain run:

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from "@theme/CodeBlock";

<Tabs>
  <TabItem value="pip" label="Pip" default>
    <CodeBlock language="bash">pip install langchain</CodeBlock>
  </TabItem>
  <TabItem value="conda" label="Conda">
    <CodeBlock language="bash">conda install langchain -c conda-forge</CodeBlock>
  </TabItem>
</Tabs>

詳細については、[インストールガイド](/docs/get_started/installation)をご覧ください。

> For more details, see our [Installation guide](/docs/get_started/installation).

### Environment | 環境設定

LangChainを使用する際には、通常、モデルプロバイダーやデータストア、APIなどとの統合が必要です。この例では、OpenAIのモデルAPIを利用します。

> Using LangChain will usually require integrations with one or more model providers, data stores, APIs, etc. For this example, we'll use OpenAI's model APIs.

まず、OpenAIのPythonパッケージをインストールする必要があります：

> First we'll need to install their Python package:

```bash
pip install openai
```

APIにアクセスするためにはAPIキーが必要で、アカウントを作成し[こちら](https://platform.openai.com/account/api-keys)から取得できます。キーを取得したら、以下のコマンドを実行して環境変数として設定しましょう：

> Accessing the API requires an API key, which you can get by creating an account and heading [here](https://platform.openai.com/account/api-keys). Once we have a key we'll want to set it as an environment variable by running:

```bash
export OPENAI_API_KEY="..."
```

環境変数を設定したくない場合は、OpenAI LLMクラスを初期化する際に、`openai_api_key`という名前のパラメータを使用して直接キーを渡すことができます。

> If you'd prefer not to set an environment variable you can pass the key in directly via the `openai_api_key` named parameter when initiating the OpenAI LLM class:

```python
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(openai_api_key="...")
```

### LangSmith | LangSmith

LangChainを用いて構築するアプリケーションは、多くのステップと複数回のLLM呼び出しを含むことが一般的です。これらのアプリケーションがさらに複雑になるにつれて、チェーンやエージェント内部で具体的に何が起きているのかを把握することが極めて重要になります。この把握を最も効果的に行う方法は、[LangSmith](https://smith.langchain.com)を利用することです。

> Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls.
> As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent.
> The best way to do this is with [LangSmith](https://smith.langchain.com).

LangSmithは必須ではありませんが、便利なツールです。LangSmithを使用する場合は、上記のリンクからサインアップした後、トレースのログを記録を開始するために環境変数を設定してください：

> Note that LangSmith is not needed, but it is helpful.
> If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:

```shell
export LANGCHAIN_TRACING_V2="true"
export LANGCHAIN_API_KEY="..."
```

### LangServe | LangServe

LangServeは開発者がLangChainチェーンをREST APIとして展開するのを支援します。LangChainを利用するためにLangServeが必須ではありませんが、このガイドではLangServeを使ってアプリケーションをデプロイする方法をご紹介します。

> LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we'll show how you can deploy your app with LangServe.

次のコマンドでインストールしてください：

> Install with:

```bash
pip install "langserve[all]"
```

## Building with LangChain | LangChainを用いたビルド

LangChainは、言語モデルアプリケーションを構築するために利用できる多数のモジュールを提供しています。これらのモジュールは、シンプルなアプリケーションでは単体で使用できるほか、より複雑なユースケースのために組み合わせて使用することも可能です。この組み合わせは**LangChain Expression Language**（LCEL）によって支えられており、LCELは多くのモジュールが実装する統一された`Runnable`インターフェースを定義しています。これにより、コンポーネントをシームレスに連携させることができます。

> LangChain provides many modules that can be used to build language model applications.
> Modules can be used as standalones in simple applications and they can be composed for more complex use cases.
> Composition is powered by **LangChain Expression Language** (LCEL), which defines a unified `Runnable` interface that many modules implement, making it possible to seamlessly chain components.

最もシンプルでありながら一般的なチェーンは、以下の三つの要素を含んでいます：

> The simplest and most common chain contains three things:

* LLM/Chat Model: 言語モデルはここでの中核的な推論エンジンです。LangChainを効果的に使用するためには、異なるタイプの言語モデルとそれらの使い方を理解することが必要です。
  > LLM/Chat Model: The language model is the core reasoning engine here. In order to work with LangChain, you need to understand the different types of language models and how to work with them.
* プロンプトテンプレート：これは言語モデルに指示を与えます。このテンプレートによって言語モデルが出力する内容が決定されるため、プロンプトの作成方法と異なるプロンプト戦略を理解することが非常に重要です。
  > Prompt Template: This provides instructions to the language model. This controls what the language model outputs, so understanding how to construct prompts and different prompting strategies is crucial.
* Output Parser: これらは、言語モデルからの生の応答をより扱いやすい形式に変換し、下流の処理で出力を容易に利用できるようにします。
  > Output Parser: These translate the raw response from the language model to a more workable format, making it easy to use the output downstream.

このガイドでは、それぞれのコンポーネントを個別に説明した後、それらをどのように組み合わせるかについて解説します。これらの概念を理解することは、LangChainアプリケーションを効果的に使用し、カスタマイズするための良い基盤となります。多くのLangChainアプリケーションでは、モデルやプロンプトの設定が可能ですので、これらの機能を最大限に活用する方法を知ることが、大きな助けになります。

> In this guide we'll cover those three components individually, and then go over how to combine them.
> Understanding these concepts will set you up well for being able to use and customize LangChain applications.
> Most LangChain applications allow you to configure the model and/or the prompt, so knowing how to take advantage of this will be a big enabler.

### LLM / Chat Model | LLM / チャットモデル

言語モデルには二つのタイプがあります：

> There are two types of language models:

* `LLM`：この基礎となるモデルは文字列を入力として受け取り、文字列を返します
  > `LLM`: underlying model takes a string as input and returns a string
* `ChatModel`：このモデルはメッセージのリストを入力として受け取り、メッセージを返します
  > `ChatModel`: underlying model takes a list of messages as input and returns a message

文字列はシンプルですが、具体的に「メッセージ」とは何でしょうか？基本的なメッセージインターフェースは`BaseMessage`によって定義され、2つの必須属性を持っています：

> Strings are simple, but what exactly are messages? The base message interface is defined by `BaseMessage`, which has two required attributes:

* `content`: メッセージの内容。通常は文字列です。
  > `content`: The content of the message. Usually a string.
* `role`：`BaseMessage`が発信されたエンティティを示します。
  > `role`: The entity from which the `BaseMessage` is coming.

LangChainは、異なる役割を簡単に識別するための複数のオブジェクトを提供しています：

> LangChain provides several objects to easily distinguish between different roles:

* `HumanMessage`: 人間またはユーザーから送信される`BaseMessage`。
  > `HumanMessage`: A `BaseMessage` coming from a human/user.
* `AIMessage`: AI/アシスタントから発信される`BaseMessage`。
  > `AIMessage`: A `BaseMessage` coming from an AI/assistant.
* `SystemMessage`: システムから送出される`BaseMessage`。
  > `SystemMessage`: A `BaseMessage` coming from the system.
* `FunctionMessage` / `ToolMessage`: 関数やツールの呼び出し結果を含む`BaseMessage`。
  > `FunctionMessage` / `ToolMessage`: A `BaseMessage` containing the output of a function or tool call.

これらの役割に適切なものがない場合、`ChatMessage` クラスを使用して役割を手動で指定することができます。

> If none of those roles sound right, there is also a `ChatMessage` class where you can specify the role manually.

LangChainは、`LLM`と`ChatModel`の両方に共通のインターフェースを提供しています。しかし、特定の言語モデルに対して最も効果的にプロンプトを構築するためには、両者の違いを理解することが役立ちます。

> LangChain provides a common interface that's shared by both `LLM`s and `ChatModel`s.
> However it's useful to understand the difference in order to most effectively construct prompts for a given language model.

`LLM`や`ChatModel`を呼び出す最もシンプルな方法は、すべてのLangChain Expression Language (LCEL)オブジェクトに共通の同期呼び出しメソッドである`.invoke()`を使用することです。

> The simplest way to call an `LLM` or `ChatModel` is using `.invoke()`, the universal synchronous call method for all LangChain Expression Language (LCEL) objects:

* `LLM.invoke`: 文字列を入力として受け取り、文字列を返します。
  > `LLM.invoke`: Takes in a string, returns a string.
* `ChatModel.invoke`：`BaseMessage`のリストを入力として受け取り、`BaseMessage`を返します。
  > `ChatModel.invoke`: Takes in a list of `BaseMessage`, returns a `BaseMessage`.

これらのメソッドの入力タイプは実際にはもっと汎用的ですが、ここでは簡単のために、LLMは文字列のみを、チャットモデルはメッセージのリストのみを受け取ると仮定しましょう。「Go deeper」セクションをチェックして、モデルの呼び出しについてさらに詳しく学びましょう。

> The input types for these methods are actually more general than this, but for simplicity here we can assume LLMs only take strings and Chat models only takes lists of messages.
> Check out the "Go deeper" section below to learn more about model invocation.

さまざまなタイプのモデルと入力方法を使って作業する方法を見てみましょう。まずは、LLMとChatModelをインポートするところから始めましょう。

> Let's see how to work with these different types of models and these different types of inputs.
> First, let's import an LLM and a ChatModel.

```python
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

llm = OpenAI()
chat_model = ChatOpenAI()
```

`LLM`オブジェクトと`ChatModel`オブジェクトは、実質的には設定用のオブジェクトです。`temperature`などのパラメータを指定して初期化し、それらを他の場所で利用することができます。

> `LLM` and `ChatModel` objects are effectively configuration objects.
> You can initialize them with parameters like `temperature` and others, and pass them around.

```python
from langchain.schema import HumanMessage

text = "What would be a good company name for a company that makes colorful socks?"
messages = [HumanMessage(content=text)]

llm.invoke(text)
# >> Feetful of Fun

chat_model.invoke(messages)
# >> AIMessage(content="Socks O'Color")
```

<details> <summary>Go deeper</summary>

`LLM.invoke`と`ChatModel.invoke`は、実際には`Union[str, List[BaseMessage], PromptValue]`のいずれかを入力として受け付けます。`PromptValue`は、入力を文字列またはメッセージとして返すための独自のカスタムロジックを定義したオブジェクトです。`LLM`はこれらの入力を文字列に変換するロジックを持ち、`ChatModel`はこれらをメッセージに変換するロジックを持っています。`LLM`と`ChatModel`が同じ種類の入力を受け入れるという事実は、ほとんどの場合、チェーン内で互いに直接交換しても何も壊れないことを意味していますが、入力がどのように変換されるか、そしてそれがモデルのパフォーマンスにどのように影響するかを考慮することは重要です。モデルについてさらに深く掘り下げるには、[言語モデル](/docs/modules/model_io/models)セクションをご覧ください。

> `LLM.invoke` and `ChatModel.invoke` actually both support as input any of `Union[str, List[BaseMessage], PromptValue]`.
> `PromptValue` is an object that defines its own custom logic for returning its inputs either as a string or as messages.
> `LLM`s have logic for coercing any of these into a string, and `ChatModel`s have logic for coercing any of these to messages.
> The fact that `LLM` and `ChatModel` accept the same inputs means that you can directly swap them for one another in most chains without breaking anything,
> though it's of course important to think about how inputs are being coerced and how that may affect model performance.
> To dive deeper on models head to the [Language models](/docs/modules/model_io/models) section.

</details>

### Prompt templates | プロンプトテンプレート

ほとんどのLLMアプリケーションは、ユーザーの入力を直接LLMに渡すわけではありません。通常、特定のタスクに関する追加の文脈を提供するために、ユーザーの入力を「プロンプトテンプレート」と呼ばれるより大きなテキストに組み込みます。

> Most LLM applications do not pass user input directly into an LLM. Usually they will add the user input to a larger piece of text, called a prompt template, that provides additional context on the specific task at hand.

前の例で、私たちがモデルに渡したテキストには、会社名を生成する指示が含まれていました。私たちのアプリケーションでは、ユーザーがモデルへの指示を気にせずに、会社や製品の説明だけを提供できれば理想的です。

> In the previous example, the text we passed to the model contained instructions to generate a company name. For our application, it would be great if the user only had to provide the description of a company/product without worrying about giving the model instructions.

プロンプトテンプレートは、まさにこれを実現するために役立ちます！ユーザーの入力から完全にフォーマットされたプロンプトを生成するまでの全てのロジックを一まとめにします。これは非常にシンプルなものから始めることができ、例えば、上記の文字列を生成するプロンプトは単に次のようになります：

> PromptTemplates help with exactly this!
> They bundle up all the logic for going from user input into a fully formatted prompt.
> This can start off very simple - for example, a prompt to produce the above string would just be:

```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template("What is a good name for a company that makes {product}?")
prompt.format(product="colorful socks")
```

```python
What is a good name for a company that makes colorful socks?
```

しかし、生の文字列フォーマットよりもこれらを使用する利点はいくつかあります。変数を「部分的に」指定できるのです - 例えば、一度にいくつかの変数だけをフォーマットすることが可能です。さらに、異なるテンプレートを組み合わせて、簡単に一つのプロンプトを作成することができます。これらの機能についての詳細は、[プロンプトに関するセクション](/docs/modules/model_io/prompts)をご覧ください。

> However, the advantages of using these over raw string formatting are several.
> You can "partial" out variables - e.g. you can format only some of the variables at a time.
> You can compose them together, easily combining different templates into a single prompt.
> For explanations of these functionalities, see the [section on prompts](/docs/modules/model_io/prompts) for more detail.

`PromptTemplate`は、メッセージのリストを生成するのにも使えます。この場合、プロンプトにはコンテンツに関する情報だけではなく、各メッセージの役割やリスト内での位置などの情報も含まれています。ここでよくあるのは、`ChatPromptTemplate`が`ChatMessageTemplates`のリストとして機能することです。各`ChatMessageTemplate`には、その`ChatMessage`をどのように整形するかの指示が含まれており、メッセージの役割と内容を定義します。以下にその例を示します：

> `PromptTemplate`s can also be used to produce a list of messages.
> In this case, the prompt not only contains information about the content, but also each message (its role, its position in the list, etc.).
> Here, what happens most often is a `ChatPromptTemplate` is a list of `ChatMessageTemplates`.
> Each `ChatMessageTemplate` contains instructions for how to format that `ChatMessage` - its role, and then also its content.
> Let's take a look at this below:

```python
from langchain.prompts.chat import ChatPromptTemplate

template = "You are a helpful assistant that translates {input_language} to {output_language}."
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])

chat_prompt.format_messages(input_language="English", output_language="French", text="I love programming.")
```

```pycon
[
    SystemMessage(content="You are a helpful assistant that translates English to French.", additional_kwargs={}),
    HumanMessage(content="I love programming.")
]
```

ChatPromptTemplateは他の方法でも構築できます - 詳細は[promptsのセクション](/docs/modules/model_io/prompts)を参照してください。

> ChatPromptTemplates can also be constructed in other ways - see the [section on prompts](/docs/modules/model_io/prompts) for more detail.

### Output parsers | 出力パーサー

`OutputParser`は、言語モデルの生の出力を下流で使用できる形式に変換するものです。
`OutputParser`にはいくつかの主要なタイプがあり、以下にその例を挙げます：

> `OutputParser`s convert the raw output of a language model into a format that can be used downstream.
> There are a few main types of `OutputParser`s, including:

* `LLM`からのテキストを構造化された情報（例えばJSON）に変換する
  > Convert text from `LLM` into structured information (e.g. JSON)
* `ChatMessage`を単なる文字列に変換する
  > Convert a `ChatMessage` into just a string
* メッセージだけでなく、呼び出しから返される追加情報（例えばOpenAI関数の実行時のような）も文字列に変換します。
  > Convert the extra information returned from a call besides the message (like OpenAI function invocation) into a string.

この件に関する詳細情報は、[出力パーサーのセクション](/docs/modules/model_io/output_parsers)をご覧ください。

> For full information on this, see the [section on output parsers](/docs/modules/model_io/output_parsers).

この入門ガイドでは、カンマで区切られたリストをリストに変換する独自の出力パーサーを作成します。

> In this getting started guide, we will write our own output parser - one that converts a comma separated list into a list.

```python
from langchain.schema import BaseOutputParser

class CommaSeparatedListOutputParser(BaseOutputParser):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str):
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

CommaSeparatedListOutputParser().parse("hi, bye")
# >> ['hi', 'bye']
```

### Composing with LCEL | LCELを使った構成

これらすべてを一つのチェーンに組み合わせることができます。このチェーンは、入力変数を受け取り、それをプロンプトテンプレートに渡してプロンプトを生成し、そのプロンプトを言語モデルに渡した後、出力を（オプショナルな）出力パーサーを通じて処理します。これは、モジュラーなロジックをまとめる便利な方法です。実際に動作する様子を見てみましょう！

> We can now combine all these into one chain.
> This chain will take input variables, pass those to a prompt template to create a prompt, pass the prompt to a language model, and then pass the output through an (optional) output parser.
> This is a convenient way to bundle up a modular piece of logic.
> Let's see it in action!

```python
from typing import List

from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import BaseOutputParser

class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str) -> List[str]:
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

template = """You are a helpful assistant who generates comma separated lists.
A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.
ONLY return a comma separated list, and nothing more."""
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])
chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()
chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

ここで、これらのコンポーネントを結合するために `|` 構文を使用していることに注意してください。この `|` 構文はLangChain Expression Language (LCEL) によって支えられており、これらのオブジェクトが実装するユニバーサルな `Runnable` インターフェースに依存しています。LCELについてさらに詳しく知りたい場合は、[こちらのドキュメント](/docs/expression_language)をご覧ください。

> Note that we are using the `|` syntax to join these components together.
> This `|` syntax is powered by the LangChain Expression Language (LCEL) and relies on the universal `Runnable` interface that all of these objects implement.
> To learn more about LCEL, read the documentation [here](/docs/expression_language).

## Tracing with LangSmith | LangSmithを用いたトレース

最初に示した通り、環境変数を設定している場合、これまでに行ったすべてのモデルやチェーンの呼び出しは自動的にLangSmithにログとして記録されます。ログがLangSmithにあると、アプリケーションのトレースをデバッグや注釈付けを行い、それをデータセットに変換して、アプリケーションの将来のバージョンの評価に利用することができます。

> Assuming we've set our environment variables as shown in the beginning, all of the model and chain calls we've been making will have been automatically logged to LangSmith.
> Once there, we can use LangSmith to debug and annotate our application traces, then turn them into datasets for evaluating future iterations of the application.

上記のチェーンに対するトレースがどのように表示されるか、こちらで確認できます：
https://smith.langchain.com/public/09370280-4330-4eb4-a7e8-c91817f6aa13/r

> Check out what the trace for the above chain would look like:
> https://smith.langchain.com/public/09370280-4330-4eb4-a7e8-c91817f6aa13/r

LangSmithの詳細情報は[こちら](/docs/langsmith/)からご覧いただけます。

> For more on LangSmith [head here](/docs/langsmith/).

## Serving with LangServe | LangServeによるサービス提供

アプリケーションを構築したので、次はそれを配信する必要があります。そのためにLangServeが役立ちます。LangServeは、開発者がLCELチェーンをREST APIとして展開するのを支援します。このライブラリはFastAPIに統合されており、データ検証にはpydanticを利用しています。

> Now that we've built an application, we need to serve it. That's where LangServe comes in.
> LangServe helps developers deploy LCEL chains as a REST API.
> The library is integrated with FastAPI and uses pydantic for data validation.

### Server | サーバ

アプリケーションのサーバーを作成するために、次の3つの要素を含む `serve.py` ファイルを作成します：

> To create a server for our application we'll make a `serve.py` file with three things:

1. 私たちのチェーンの定義（上記と同じ）
   > The definition of our chain (same as above)
2. 私たちのFastAPIアプリ
   > Our FastAPI app
3. `langserve.add_routes`を用いてチェーンを提供するルートの定義
   > A definition of a route from which to serve the chain, which is done with `langserve.add_routes`

```python
#!/usr/bin/env python
from typing import List

from fastapi import FastAPI
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema import BaseOutputParser
from langserve import add_routes

# 1. Chain definition

class CommaSeparatedListOutputParser(BaseOutputParser[List[str]]):
    """Parse the output of an LLM call to a comma-separated list."""


    def parse(self, text: str) -> List[str]:
        """Parse the output of an LLM call."""
        return text.strip().split(", ")

template = """You are a helpful assistant who generates comma separated lists.
A user will pass in a category, and you should generate 5 objects in that category in a comma separated list.
ONLY return a comma separated list, and nothing more."""
human_template = "{text}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", template),
    ("human", human_template),
])
category_chain = chat_prompt | ChatOpenAI() | CommaSeparatedListOutputParser()

# 2. App definition
app = FastAPI(
  title="LangChain Server",
  version="1.0",
  description="A simple API server using LangChain's Runnable interfaces",
)

# 3. Adding chain route
add_routes(
    app,
    category_chain,
    path="/category_chain",
)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="localhost", port=8000)
```

それで全てです！このファイルを実行すると：

> And that's it! If we execute this file:

```bash
python serve.py
```

この手順を実行すれば、localhost:8000で私たちのチェーンが稼働しているのを確認できるはずです。

> we should see our chain being served at localhost:8000.

### Playground | プレイグラウンド

LangServeサービスには、アプリケーションの設定と起動を行い、ストリーミング出力と中間ステップを可視化するためのシンプルな組み込みUIが備わっています。http://localhost:8000/category\_chain/playground/ にアクセスして、実際にお試しください！

> Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps.
> Head to http://localhost:8000/category\_chain/playground/ to try it out!

### Client | クライアント

それでは、プログラムを通じてサービスと対話するクライアントを設定しましょう。`langserve.RemoteRunnable`を利用することで、これを簡単に実現できます。この機能を使えば、サーバー側で提供されているチェーンと、まるでクライアント側で実行しているかのように対話できます。

> Now let's set up a client for programmatically interacting with our service. We can easily do this with the `langserve.RemoteRunnable`.
> Using this, we can interact with the served chain as if it were running client-side.

```python
from langserve import RemoteRunnable

remote_chain = RemoteRunnable("http://localhost:8000/category_chain/")
remote_chain.invoke({"text": "colors"})
# >> ['red', 'blue', 'green', 'yellow', 'orange']
```

LangServeの他の多くの機能についてもっと学びたい方は[こちら](/docs/langserve)へどうぞ。

> To learn more about the many other features of LangServe [head here](/docs/langserve).

## Next steps | 次のステップ

LangChainを用いてアプリケーションを構築する方法、LangSmithでトレースする方法、そしてLangServeでサービスとして提供する方法について触れてきました。ここで紹介できる以上に、これら3つのツールには多くの機能が存在します。あなたの探求を続けるために：

> We've touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe.
> There are a lot more features in all three of these than we can cover here.
> To continue on your journey:

* [LangChain Expression Language (LCEL)](/docs/expression_language)を読んで、これらのコンポーネントをどのように連結させるかを学びましょう
  > Read up on [LangChain Expression Language (LCEL)](/docs/expression_language) to learn how to chain these components together
* LLM（大規模言語モデル）、プロンプト、出力パーサーについてさらに深く掘り下げ、その他の[主要なコンポーネント](/docs/modules)について学びましょう。
  > [Dive deeper](/docs/modules/model_io) into LLMs, prompts, and output parsers and learn the other [key components](/docs/modules)
* 一般的なエンドツーエンドのユースケースやテンプレートアプリケーションを探求する
  > Explore common [end-to-end use cases](/docs/use_cases/qa_structured/sql) and [template applications](/docs/templates)
* [LangSmithについて読む](/docs/langsmith/)、デバッグ、テスト、モニタリングなど多岐にわたる機能を備えたプラットフォームです。
  > [Read up on LangSmith](/docs/langsmith/), the platform for debugging, testing, monitoring and more
* [LangServe](/docs/langserve)を利用してアプリケーションを運用する方法をさらに学びましょう。
  > Learn more about serving your applications with [LangServe](/docs/langserve)
