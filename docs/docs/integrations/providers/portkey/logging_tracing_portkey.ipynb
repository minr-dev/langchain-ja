{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log, Trace, and Monitor | „É≠„Ç∞„ÄÅ„Éà„É¨„Éº„Çπ„ÄÅÁõ£Ë¶ñ",
    "",
    "Langchain„Çí‰ΩøÁî®„Åó„Å¶„Ç¢„Éó„É™„ÇÑ„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÇíÊßãÁØâ„Åô„ÇãÈöõ„ÄÅÂçò‰∏Ä„ÅÆ„É¶„Éº„Ç∂„Éº„É™„ÇØ„Ç®„Çπ„Éà„ÇíÊ∫Ä„Åü„Åô„Åü„ÇÅ„Å´„ÅØË§áÊï∞„ÅÆAPI„Ç≥„Éº„É´„ÅåÂøÖË¶Å„Å´„Å™„Çä„Åæ„Åô„ÄÇ„Åó„Åã„Åó„ÄÅ„Åì„Çå„Çâ„ÅÆ„É™„ÇØ„Ç®„Çπ„Éà„ÅØÂàÜÊûê„Åô„ÇãÈöõ„Å´„ÅØÈÄ£Êê∫„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ[**Portkey**](/docs/ecosystem/integrations/portkey)„ÇíÂà©Áî®„Åô„Çã„Å®„ÄÅÂçò‰∏Ä„ÅÆ„É¶„Éº„Ç∂„Éº„É™„ÇØ„Ç®„Çπ„Éà„Å´Èñ¢ÈÄ£„Åô„ÇãÂÖ®„Å¶„ÅÆÂüã„ÇÅËæº„Åø„ÄÅÂÆå‰∫Ü„ÄÅ„Åù„ÅÆ‰ªñ„ÅÆ„É™„ÇØ„Ç®„Çπ„Éà„Åå„É≠„Ç∞„Å´Ë®òÈå≤„Åï„Çå„ÄÅÂÖ±ÈÄö„ÅÆID„ÅßËøΩË∑°„Åï„Çå„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ„É¶„Éº„Ç∂„Éº„ÅÆ„Ç§„É≥„Çø„É©„ÇØ„Ç∑„Éß„É≥„ÇíÂÆåÂÖ®„Å´ÂèØË¶ñÂåñ„Åô„Çã„Åì„Å®„ÅåÂèØËÉΩ„Å´„Å™„Çä„Åæ„Åô„ÄÇ",
    "",
    "> When building apps or agents using Langchain, you end up making multiple API calls to fulfill a single user request. However, these requests are not chained when you want to analyse them. With [**Portkey**](/docs/ecosystem/integrations/portkey), all the embeddings, completion, and other requests from a single user request will get logged and traced to a common ID, enabling you to gain full visibility of user interactions.",
    "",
    "„Åì„ÅÆ„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÅØ„ÄÅLangchain„Ç¢„Éó„É™„Åß`Portkey`„Çí‰ΩøÁî®„Åó„Å¶Langchain LLM„ÅÆÂëº„Å≥Âá∫„Åó„Çí„É≠„Ç∞„Å´Ë®òÈå≤„Åó„ÄÅËøΩË∑°„Åó„ÄÅÁõ£Ë¶ñ„Åô„ÇãÊñπÊ≥ï„Å´„Å§„ÅÑ„Å¶„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„Éê„Ç§„Çπ„ÉÜ„ÉÉ„Éó„Ç¨„Ç§„Éâ„Å®„Åó„Å¶Ê©üËÉΩ„Åó„Åæ„Åô„ÄÇ",
    "",
    "> This notebook serves as a step-by-step guide on how to log, trace, and monitor Langchain LLM calls using `Portkey` in your Langchain app.",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "„Åæ„Åö„ÄÅPortkey„ÄÅOpenAI„ÄÅ„Åù„Åó„Å¶Agent„ÉÑ„Éº„É´„Çí„Ç§„É≥„Éù„Éº„Éà„Åó„Åæ„Åó„Çá„ÅÜ",
    "",
    "> First, let's import Portkey, OpenAI, and Agent tools",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import Portkey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‰ª•‰∏ã„Å´„ÅÇ„Å™„Åü„ÅÆOpenAI API„Ç≠„Éº„ÇíË≤º„Çä‰ªò„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ[(„Åì„Å°„Çâ„ÅßË¶ã„Å§„Åë„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô)](https://platform.openai.com/account/api-keys)",
    "",
    "> Paste your OpenAI API key below. [(You can find it here)](https://platform.openai.com/account/api-keys)",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"<OPENAI_API_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Portkey API Key | Portkey API„Ç≠„Éº„ÇíÂèñÂæó„Åô„Çã",
    "",
    "1. [„Åì„Å°„Çâ„Åã„ÇâPortkey„Å´ÁôªÈå≤„Åó„Å¶„Åè„Å†„Åï„ÅÑ](https://app.portkey.ai/login)",
    "   > Sign up for [Portkey here](https://app.portkey.ai/login)",
    "2. [„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ](https://app.portkey.ai/)„Åß„ÄÅÂ∑¶‰∏ä„Å´„ÅÇ„Çã„Éó„É≠„Éï„Ç£„Éº„É´„Ç¢„Ç§„Ç≥„É≥„Çí„ÇØ„É™„ÉÉ„ÇØ„Åó„ÄÅ„Åù„ÅÆÂæå„ÄåAPI„Ç≠„Éº„Çí„Ç≥„Éî„Éº„Äç„Çí„ÇØ„É™„ÉÉ„ÇØ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
    "   > On your [dashboard](https://app.portkey.ai/), click on the profile icon on the top left, then click on \"Copy API Key\"",
    "3. ‰ª•‰∏ã„Å´Ë≤º„Çä‰ªò„Åë„Å¶„Åè„Å†„Åï„ÅÑ",
    "   > Paste it below",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PORTKEY_API_KEY = \"<PORTKEY_API_KEY>\"  # Paste your Portkey API Key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Trace ID | „Éà„É¨„Éº„ÇπID„ÇíË®≠ÂÆö",
    "",
    "1. ‰ª•‰∏ã„Å´„ÅÇ„Å™„Åü„ÅÆ„É™„ÇØ„Ç®„Çπ„Éà„ÅÆ„Éà„É¨„Éº„ÇπID„ÇíË®≠ÂÆö„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
    "   > Set the trace id for your request below",
    "2. Âçò‰∏Ä„ÅÆ„É™„ÇØ„Ç®„Çπ„Éà„Åã„ÇâÁô∫Áîü„Åô„Çã„Åô„Åπ„Å¶„ÅÆAPI„Ç≥„Éº„É´„Å´ÂØæ„Åó„Å¶„ÄÅTrace ID„ÅåÂÖ±ÈÄö„Åß„ÅÇ„Çã„Åì„Å®„Åå„ÅÇ„Çä„Åæ„Åô",
    "   > The Trace ID can be common for all API calls originating from a single request",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACE_ID = \"portkey_langchain_demo\"  # Set trace id here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Portkey Headers | Portkey „Éò„ÉÉ„ÉÄ„Éº„ÇíÁîüÊàê„Åô„Çã",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = Portkey.Config(\n",
    "    api_key=PORTKEY_API_KEY,\n",
    "    trace_id=TRACE_ID,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÇíÈÄöÂ∏∏ÈÄö„ÇäÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ**ÂîØ‰∏Ä„ÅÆÂ§âÊõ¥ÁÇπ**„ÅØ„ÄÅ„Åì„Çå„Åã„Çâ„É™„ÇØ„Ç®„Çπ„Éà„Å´**‰∏äË®ò„ÅÆ„Éò„ÉÉ„ÉÄ„Éº„ÇíÂê´„ÇÅ„Çã**„Åì„Å®„Åß„Åô„ÄÇ",
    "",
    "> Run your agent as usual. The **only** change is that we will **include the above headers** in the request now.",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, headers=headers)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")\n",
    "\n",
    "# Let's test it out!\n",
    "agent.run(\n",
    "    \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Logging & Tracing Works on Portkey | Portkey„Åß„ÅÆ„É≠„Ç∞Ë®òÈå≤„Å®„Éà„É¨„Éº„Çπ„ÅÆ‰ªïÁµÑ„Åø",
    "",
    "**„É≠„Ç∞Ë®òÈå≤**",
    "",
    "> **Logging**",
    "",
    "* Portkey„ÇíÈÄö„Åò„Å¶„É™„ÇØ„Ç®„Çπ„Éà„ÇíÈÄÅ‰ø°„Åô„Çã„Åì„Å®„Åß„ÄÅ„Éá„Éï„Ç©„É´„Éà„ÅßÂÖ®„Å¶„ÅÆ„É™„ÇØ„Ç®„Çπ„Éà„ÅåË®òÈå≤„Åï„Çå„Åæ„Åô",
    "  > Sending your request through Portkey ensures that all of the requests are logged by default",
    "* ÂêÑ„É™„ÇØ„Ç®„Çπ„Éà„É≠„Ç∞„Å´„ÅØ„ÄÅ`timestamp`Ôºà„Çø„Ç§„É†„Çπ„Çø„É≥„ÉóÔºâ„ÄÅ`model name`Ôºà„É¢„Éá„É´ÂêçÔºâ„ÄÅ`total cost`ÔºàÁ∑è„Ç≥„Çπ„ÉàÔºâ„ÄÅ`request time`Ôºà„É™„ÇØ„Ç®„Çπ„ÉàÊôÇÈñìÔºâ„ÄÅ`request json`Ôºà„É™„ÇØ„Ç®„Çπ„ÉàJSONÔºâ„ÄÅ`response json`Ôºà„É¨„Çπ„Éù„É≥„ÇπJSONÔºâ„ÄÅ„Åä„Çà„Å≥ËøΩÂä†„ÅÆPortkeyÊ©üËÉΩ„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô",
    "  > Each request log contains `timestamp`, `model name`, `total cost`, `request time`, `request json`, `response json`, and additional Portkey features",
    "",
    "**„Éà„É¨„Éº„Ç∑„É≥„Ç∞**",
    "",
    "> **Tracing**",
    "",
    "* „Éà„É¨„Éº„ÇπID„ÅØÂêÑ„É™„ÇØ„Ç®„Çπ„Éà„Å´Ê∑ª‰ªò„Åï„Çå„ÄÅPortkey„ÉÄ„ÉÉ„Ç∑„É•„Éú„Éº„Éâ„ÅÆ„É≠„Ç∞„Å´Ë°®Á§∫„Åï„Çå„Åæ„Åô",
    "  > Trace id is passed along with each request and is visibe on the logs on Portkey dashboard",
    "* „Åæ„Åü„ÄÅÂøÖË¶Å„Åß„ÅÇ„Çå„Å∞„ÄÅÂêÑ„É™„ÇØ„Ç®„Çπ„Éà„Å´**Âõ∫Êúâ„ÅÆ„Éà„É¨„Éº„ÇπID**„ÇíË®≠ÂÆö„Åô„Çã„Åì„Å®„ÇÇ„Åß„Åç„Åæ„Åô",
    "  > You can also set a **distinct trace id** for each request if you want",
    "* „É¶„Éº„Ç∂„Éº„Éï„Ç£„Éº„Éâ„Éê„ÉÉ„ÇØ„Çí„Éà„É¨„Éº„ÇπID„Å´„ÇÇËøΩÂä†„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ[„Åì„ÅÆ‰ª∂„ÅÆË©≥Á¥∞„ÅØ„Åì„Å°„Çâ](https://docs.portkey.ai/key-features/feedback-api)",
    "  > You can append user feedback to a trace id as well. [More info on this here](https://docs.portkey.ai/key-features/feedback-api)",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced LLMOps Features - Caching, Tagging, Retries | È´òÂ∫¶„Å™LLMOps„ÅÆÊ©üËÉΩ - „Ç≠„É£„ÉÉ„Ç∑„É≥„Ç∞„ÄÅ„Çø„Ç∞‰ªò„Åë„ÄÅ„É™„Éà„É©„Ç§",
    "",
    "„É≠„Ç∞Ë®òÈå≤„ÇÑ„Éà„É¨„Éº„Ç∑„É≥„Ç∞„Å´Âä†„Åà„Å¶„ÄÅPortkey„ÅØÊó¢Â≠ò„ÅÆ„ÉØ„Éº„ÇØ„Éï„É≠„Éº„Å´ÁîüÁî£Ê©üËÉΩ„ÇíËøΩÂä†„Åô„ÇãËøΩÂä†„ÅÆÊ©üËÉΩ„ÇÇÊèê‰æõ„Åó„Åæ„ÅôÔºö",
    "",
    "> In addition to logging and tracing, Portkey provides more features that add production capabilities to your existing workflows:",
    "",
    "**„Ç≠„É£„ÉÉ„Ç∑„É≥„Ç∞**",
    "",
    "> **Caching**",
    "",
    "‰ª•Ââç„Å´ÂØæÂøú„Åó„ÅüÈ°ßÂÆ¢„ÅÆÂïè„ÅÑÂêà„Çè„Åõ„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅOpenAI„Å´ÂÜçÂ∫¶ÈÄÅ‰ø°„Åô„Çã‰ª£„Çè„Çä„Å´„Ç≠„É£„ÉÉ„Ç∑„É•„Åã„ÇâÂøúÁ≠î„Åó„Åæ„Åô„ÄÇÂÆåÂÖ®„Å´‰∏ÄËá¥„Åô„ÇãÊñáÂ≠óÂàó„Åæ„Åü„ÅØÊÑèÂë≥ÁöÑ„Å´È°û‰ºº„Åó„ÅüÊñáÂ≠óÂàó„Å´„Éû„ÉÉ„ÉÅ„Åó„Åæ„Åô„ÄÇ„Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÂà©Áî®„Åô„Çã„Åì„Å®„Åß„Ç≥„Çπ„Éà„ÇíÂâäÊ∏õ„Åó„ÄÅ„É¨„Ç§„ÉÜ„É≥„Ç∑„Éº„Çí20ÂÄçÁü≠Á∏Æ„Åß„Åç„Åæ„Åô„ÄÇ",
    "",
    "> Respond to previously served customers queries from cache instead of sending them again to OpenAI. Match exact strings OR semantically similar strings. Cache can save costs and reduce latencies by 20x.",
    "",
    "**„É™„Éà„É©„Ç§**",
    "",
    "> **Retries**",
    "",
    "Â§±Êïó„Åó„ÅüAPI„É™„ÇØ„Ç®„Çπ„Éà„ÇíËá™ÂãïÁöÑ„Å´**ÊúÄÂ§ß5Âõû**ÂÜçÂá¶ÁêÜ„Åó„Åæ„Åô„ÄÇ„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÅÆÈÅéË≤†Ëç∑„ÇíÈò≤„Åê„Åü„ÇÅ„Å´„ÄÅ„É™„Éà„É©„Ç§Ë©¶Ë°åÈñì„ÅÆÈñìÈöî„ÇíÂ∫É„Åí„Çã**ÊåáÊï∞„Éê„ÉÉ„ÇØ„Ç™„Éï**Êà¶Áï•„Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ",
    "",
    "> Automatically reprocess any unsuccessful API requests **`upto 5`** times. Uses an **`exponential backoff`** strategy, which spaces out retry attempts to prevent network overload.",
    "",
    "| Ê©üËÉΩ | Ë®≠ÂÆö„Ç≠„Éº | ÂÄ§Ôºà„Çø„Ç§„ÉóÔºâ |",
    "| -- | -- | -- |",
    "| [üîÅ Ëá™Âãï„É™„Éà„É©„Ç§](https://docs.portkey.ai/key-features/automatic-retries) | `retry_count` | `integer` \\[1,2,3,4,5] |",
    "| [üß† „Ç≠„É£„ÉÉ„Ç∑„É•„ÇíÊúâÂäπ„Å´„Åô„Çã](https://docs.portkey.ai/key-features/request-caching) | `cache` | `simple` „Åæ„Åü„ÅØ `semantic` |",
    "",
    "> | Feature | Config Key | Value (Type) |",
    "> | -- | -- | -- |",
    "> | [üîÅ Automatic Retries](https://docs.portkey.ai/key-features/automatic-retries) | `retry_count` | `integer` \\[1,2,3,4,5] |",
    "> | [üß† Enabling Cache](https://docs.portkey.ai/key-features/request-caching) | `cache` | `simple` OR `semantic` |",
    "",
    "**„Çø„Ç∞‰ªò„Åë**",
    "",
    "> **Tagging**",
    "",
    "‰∫ãÂâç„Å´ÂÆöÁæ©„Åï„Çå„Åü„Çø„Ç∞„Çí‰ΩøÁî®„Åó„Å¶„ÄÅÂêÑ„É¶„Éº„Ç∂„Éº„ÅÆ„Ç§„É≥„Çø„É©„ÇØ„Ç∑„Éß„É≥„ÇíÈ´ò„ÅÑË©≥Á¥∞Â∫¶„ÅßËøΩË∑°„Åä„Çà„Å≥Áõ£Êüª„Åó„Åæ„Åô„ÄÇ",
    "",
    "> Track and audit ach user interaction in high detail with predefined tags.",
    "",
    "| „Çø„Ç∞ | Ë®≠ÂÆö„Ç≠„Éº | ÂÄ§Ôºà„Çø„Ç§„ÉóÔºâ |",
    "| -- | -- | -- |",
    "| „É¶„Éº„Ç∂„Éº„Çø„Ç∞ | `user` | `string` |",
    "| ÁµÑÁπî„Çø„Ç∞ | `organisation` | `string` |",
    "| Áí∞Â¢É„Çø„Ç∞ | `environment` | `string` |",
    "| „Éó„É≠„É≥„Éó„Éà„Çø„Ç∞Ôºà„Éê„Éº„Ç∏„Éß„É≥/ID/ÊñáÂ≠óÂàóÔºâ | `prompt` | `string` |",
    "",
    "> | Tag | Config Key | Value (Type) |",
    "> | -- | -- | -- |",
    "> | User Tag | `user` | `string` |",
    "> | Organisation Tag | `organisation` | `string` |",
    "> | Environment Tag | `environment` | `string` |",
    "> | Prompt Tag (version/id/string) | `prompt` | `string` |",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example With All Features | „Åô„Åπ„Å¶„ÅÆÊ©üËÉΩ„ÇíÂê´„ÇÄ„Ç≥„Éº„Éâ‰æã",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = Portkey.Config(\n",
    "    # Mandatory\n",
    "    api_key=\"<PORTKEY_API_KEY>\",\n",
    "    # Cache Options\n",
    "    cache=\"semantic\",\n",
    "    cache_force_refresh=\"True\",\n",
    "    cache_age=1729,\n",
    "    # Advanced\n",
    "    retry_count=5,\n",
    "    trace_id=\"langchain_agent\",\n",
    "    # Metadata\n",
    "    environment=\"production\",\n",
    "    user=\"john\",\n",
    "    organisation=\"acme\",\n",
    "    prompt=\"Frost\",\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0.9, headers=headers)\n",
    "\n",
    "print(llm(\"Two roads diverged in the yellow woods\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}